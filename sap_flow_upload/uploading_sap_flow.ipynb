{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Sap Flow data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am just trying to figure out how to store Sapflow data.\n",
    "\n",
    "Be careful with this script as it connects to the metacatalog database. The script connects to the 'default' database in metacatalog. If you want to upload the data to another (local) database, replace 'default' in `CONNECTION = 'default'` with the name of a connection you saved in the metacatalog cli. Also, Metadata and data is uploaded, which might cause duplicates. In most cases something like:\n",
    "\n",
    "```python\n",
    "UPLOAD = False\n",
    "if UPLOAD:\n",
    "    datamodels = do_upload_stuff_function()\n",
    "else:\n",
    "    datamodels = load_from_db_stuff()\n",
    "```\n",
    "\n",
    "is implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD  = True\n",
    "MIGRATE = False   # use only and only once if you have a metacatalog < 0.2 inited db\n",
    "CONNECTION = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from metacatalog import api, ext, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'metacatalog.ext.io.extension.IOExtension'>\n"
     ]
    }
   ],
   "source": [
    "# check if the IO extension is activate\n",
    "try:\n",
    "    print(ext.extension('io'))\n",
    "except AttributeError:\n",
    "    ext.activate_extension('io', 'metacatalog.ext.io', 'IOExtension')\n",
    "    from metacatalog.ext.io import IOExtension\n",
    "    ext.extension('io', IOExtension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52560, 58)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('HH_derived_sapflow_2015_VPD_10min.level1.csv', parse_dates=[0])\n",
    "df.set_index('Date Time', inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SFD022_20_E [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD022_20_E_f</th>\n",
       "      <th>SFD022_20_NW [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD022_20_NW_f</th>\n",
       "      <th>SFD029_20_N [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD029_20_N_f</th>\n",
       "      <th>SFD033_20_NE [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD033_20_NE_f</th>\n",
       "      <th>SFD048_20_N [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD048_20_N_f</th>\n",
       "      <th>...</th>\n",
       "      <th>SFD233_20_N [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD233_20_N_f</th>\n",
       "      <th>SFD236_20_N [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD236_20_N_f</th>\n",
       "      <th>SFD282_20_N [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD282_20_N_f</th>\n",
       "      <th>SFD301_20_N [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD301_20_N_f</th>\n",
       "      <th>SFD301_20_W [cm**3/(cm**2 10min**1)]</th>\n",
       "      <th>SFD301_20_W_f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-22 15:10:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-22 15:20:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-22 15:30:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-22 15:40:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-22 15:50:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     SFD022_20_E [cm**3/(cm**2 10min**1)]  SFD022_20_E_f  \\\n",
       "Date Time                                                                  \n",
       "2015-04-22 15:10:00                                   NaN            NaN   \n",
       "2015-04-22 15:20:00                                   0.0          902.0   \n",
       "2015-04-22 15:30:00                                   0.0          902.0   \n",
       "2015-04-22 15:40:00                                   0.0          902.0   \n",
       "2015-04-22 15:50:00                                   0.0          902.0   \n",
       "\n",
       "                     SFD022_20_NW [cm**3/(cm**2 10min**1)]  SFD022_20_NW_f  \\\n",
       "Date Time                                                                    \n",
       "2015-04-22 15:10:00                                    NaN             NaN   \n",
       "2015-04-22 15:20:00                                    NaN             NaN   \n",
       "2015-04-22 15:30:00                                    NaN             NaN   \n",
       "2015-04-22 15:40:00                                    NaN             NaN   \n",
       "2015-04-22 15:50:00                                    NaN             NaN   \n",
       "\n",
       "                     SFD029_20_N [cm**3/(cm**2 10min**1)]  SFD029_20_N_f  \\\n",
       "Date Time                                                                  \n",
       "2015-04-22 15:10:00                                   0.0          902.0   \n",
       "2015-04-22 15:20:00                                   0.0          902.0   \n",
       "2015-04-22 15:30:00                                   0.0          902.0   \n",
       "2015-04-22 15:40:00                                   0.0          902.0   \n",
       "2015-04-22 15:50:00                                   0.0          902.0   \n",
       "\n",
       "                     SFD033_20_NE [cm**3/(cm**2 10min**1)]  SFD033_20_NE_f  \\\n",
       "Date Time                                                                    \n",
       "2015-04-22 15:10:00                                    NaN             NaN   \n",
       "2015-04-22 15:20:00                                    NaN             NaN   \n",
       "2015-04-22 15:30:00                                    NaN             NaN   \n",
       "2015-04-22 15:40:00                                    NaN             NaN   \n",
       "2015-04-22 15:50:00                                    NaN             NaN   \n",
       "\n",
       "                     SFD048_20_N [cm**3/(cm**2 10min**1)]  SFD048_20_N_f  ...  \\\n",
       "Date Time                                                                 ...   \n",
       "2015-04-22 15:10:00                                   NaN            NaN  ...   \n",
       "2015-04-22 15:20:00                                   NaN            NaN  ...   \n",
       "2015-04-22 15:30:00                                   NaN            NaN  ...   \n",
       "2015-04-22 15:40:00                                   NaN            NaN  ...   \n",
       "2015-04-22 15:50:00                                   NaN            NaN  ...   \n",
       "\n",
       "                     SFD233_20_N [cm**3/(cm**2 10min**1)]  SFD233_20_N_f  \\\n",
       "Date Time                                                                  \n",
       "2015-04-22 15:10:00                                   NaN            NaN   \n",
       "2015-04-22 15:20:00                                   NaN            NaN   \n",
       "2015-04-22 15:30:00                                   NaN            NaN   \n",
       "2015-04-22 15:40:00                                   NaN            NaN   \n",
       "2015-04-22 15:50:00                                   NaN            NaN   \n",
       "\n",
       "                     SFD236_20_N [cm**3/(cm**2 10min**1)]  SFD236_20_N_f  \\\n",
       "Date Time                                                                  \n",
       "2015-04-22 15:10:00                                   NaN            NaN   \n",
       "2015-04-22 15:20:00                                   NaN            NaN   \n",
       "2015-04-22 15:30:00                                   NaN            NaN   \n",
       "2015-04-22 15:40:00                                   NaN            NaN   \n",
       "2015-04-22 15:50:00                                   NaN            NaN   \n",
       "\n",
       "                     SFD282_20_N [cm**3/(cm**2 10min**1)]  SFD282_20_N_f  \\\n",
       "Date Time                                                                  \n",
       "2015-04-22 15:10:00                                   NaN            NaN   \n",
       "2015-04-22 15:20:00                                   NaN            NaN   \n",
       "2015-04-22 15:30:00                                   NaN            NaN   \n",
       "2015-04-22 15:40:00                                   NaN            NaN   \n",
       "2015-04-22 15:50:00                                   NaN            NaN   \n",
       "\n",
       "                     SFD301_20_N [cm**3/(cm**2 10min**1)]  SFD301_20_N_f  \\\n",
       "Date Time                                                                  \n",
       "2015-04-22 15:10:00                                   NaN            NaN   \n",
       "2015-04-22 15:20:00                                   NaN            NaN   \n",
       "2015-04-22 15:30:00                                   NaN            NaN   \n",
       "2015-04-22 15:40:00                                   NaN            NaN   \n",
       "2015-04-22 15:50:00                                   NaN            NaN   \n",
       "\n",
       "                     SFD301_20_W [cm**3/(cm**2 10min**1)]  SFD301_20_W_f  \n",
       "Date Time                                                                 \n",
       "2015-04-22 15:10:00                                   NaN            NaN  \n",
       "2015-04-22 15:20:00                                   NaN            NaN  \n",
       "2015-04-22 15:30:00                                   NaN            NaN  \n",
       "2015-04-22 15:40:00                                   NaN            NaN  \n",
       "2015-04-22 15:50:00                                   NaN            NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df <= -9999] = np.NaN\n",
    "df.dropna(how='all', axis=0, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29779, 58)\n",
      "(29779, 58)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.dropna(how='all', axis=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no empty columns; Select what seems to be one sensor and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2628fe93208>"
       "<AxesSubplot:xlabel='Date Time'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,0:2].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_f` pretty much looks like a flag. Load unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan, 902.,   9.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,1].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for all comuns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [902.0, 9.0, nan],\n",
       " [nan, 902.0, 9.0, 901.0, 90102.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0, 90002.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0, 901.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 9.0, 902.0],\n",
       " [nan, 9.0, 902.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0],\n",
       " [nan, 902.0, 9.0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[float(_) for _ in df.iloc[:,i].unique()] for i in range(1, len(df.columns), 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(911, 52)\n",
      "(904, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexd/opt/anaconda3/envs/mc_develop/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "meta = pd.read_excel('CHS-measurements.xlsx', sheet_name='Logger Hohes Holz')\n",
    "print(meta.shape)\n",
    "meta.dropna(how='all', axis=0, inplace=True)\n",
    "print(meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable description</th>\n",
       "      <th>counter</th>\n",
       "      <th>sensor type</th>\n",
       "      <th>depth/height (m)</th>\n",
       "      <th>sap wood depth (cm)</th>\n",
       "      <th>species (engl.)</th>\n",
       "      <th>DBH (for each year)\\n[cm]</th>\n",
       "      <th>date start</th>\n",
       "      <th>date end</th>\n",
       "      <th>logger/data acquisition</th>\n",
       "      <th>...</th>\n",
       "      <th>Flag_3</th>\n",
       "      <th>Flag_4</th>\n",
       "      <th>Flag_5</th>\n",
       "      <th>Flag_6</th>\n",
       "      <th>Flag_7</th>\n",
       "      <th>Flag_8</th>\n",
       "      <th>Alert_1</th>\n",
       "      <th>Alert_2</th>\n",
       "      <th>Alert_3</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>sap flow tree 022- 20mm- east</td>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oak</td>\n",
       "      <td>53.04</td>\n",
       "      <td>2015-04-16 14:30:00</td>\n",
       "      <td>2015-11-03 12:10:00</td>\n",
       "      <td>BC1</td>\n",
       "      <td>...</td>\n",
       "      <td>limits</td>\n",
       "      <td>mvdiff_max=0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>sap flow tree 022- 20mm- north-east</td>\n",
       "      <td>181.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oak</td>\n",
       "      <td>52.60</td>\n",
       "      <td>2014-04-15 11:40:00</td>\n",
       "      <td>2015-04-16 14:10:00</td>\n",
       "      <td>BC1</td>\n",
       "      <td>...</td>\n",
       "      <td>limits</td>\n",
       "      <td>mvdiff_max=0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>sap flow tree 022- 20mm- north-west</td>\n",
       "      <td>182.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oak</td>\n",
       "      <td>53.04</td>\n",
       "      <td>2015-04-16 14:30:00</td>\n",
       "      <td>2015-11-03 12:10:00</td>\n",
       "      <td>BC1</td>\n",
       "      <td>...</td>\n",
       "      <td>limits</td>\n",
       "      <td>mvdiff_max=0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>sap flow tree 22 east-north-east first install...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oak</td>\n",
       "      <td>55.81</td>\n",
       "      <td>2019-04-03 12:50:00</td>\n",
       "      <td>2019-04-30 14:10:00</td>\n",
       "      <td>BC1</td>\n",
       "      <td>...</td>\n",
       "      <td>limits</td>\n",
       "      <td>mvdiff_max=0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>sap flow tree 22 east-north-east second instal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oak</td>\n",
       "      <td>55.81</td>\n",
       "      <td>2019-04-03 12:50:00</td>\n",
       "      <td>2019-04-30 14:10:00</td>\n",
       "      <td>BC1</td>\n",
       "      <td>...</td>\n",
       "      <td>limits</td>\n",
       "      <td>mvdiff_max=0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  variable description  counter sensor type  \\\n",
       "212                      sap flow tree 022- 20mm- east    180.0         NaN   \n",
       "213                sap flow tree 022- 20mm- north-east    181.0         NaN   \n",
       "214                sap flow tree 022- 20mm- north-west    182.0         NaN   \n",
       "215  sap flow tree 22 east-north-east first install...      NaN         NaN   \n",
       "216  sap flow tree 22 east-north-east second instal...      NaN         NaN   \n",
       "\n",
       "     depth/height (m)  sap wood depth (cm) species (engl.)  \\\n",
       "212               1.3                  NaN             oak   \n",
       "213               1.3                  NaN             oak   \n",
       "214               1.3                  NaN             oak   \n",
       "215               1.3                  NaN             oak   \n",
       "216               1.3                  NaN             oak   \n",
       "\n",
       "     DBH (for each year)\\n[cm]          date start            date end  \\\n",
       "212                      53.04 2015-04-16 14:30:00 2015-11-03 12:10:00   \n",
       "213                      52.60 2014-04-15 11:40:00 2015-04-16 14:10:00   \n",
       "214                      53.04 2015-04-16 14:30:00 2015-11-03 12:10:00   \n",
       "215                      55.81 2019-04-03 12:50:00 2019-04-30 14:10:00   \n",
       "216                      55.81 2019-04-03 12:50:00 2019-04-30 14:10:00   \n",
       "\n",
       "    logger/data acquisition  ...  Flag_3           Flag_4 Flag_5 Flag_6  \\\n",
       "212                     BC1  ...  limits  mvdiff_max=0.05    NaN    NaN   \n",
       "213                     BC1  ...  limits  mvdiff_max=0.05    NaN    NaN   \n",
       "214                     BC1  ...  limits  mvdiff_max=0.05    NaN    NaN   \n",
       "215                     BC1  ...  limits  mvdiff_max=0.05    NaN    NaN   \n",
       "216                     BC1  ...  limits  mvdiff_max=0.05    NaN    NaN   \n",
       "\n",
       "    Flag_7 Flag_8 Alert_1 Alert_2 Alert_3 comment  \n",
       "212    NaN    NaN     NaN     NaN     NaN     NaN  \n",
       "213    NaN    NaN     NaN     NaN     NaN     NaN  \n",
       "214    NaN    NaN     NaN     NaN     NaN     NaN  \n",
       "215    NaN    NaN     NaN     NaN     NaN     NaN  \n",
       "216    NaN    NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [i for i,s in enumerate(meta['variable description'].values) if 'sap' in s]\n",
    "saps = meta.iloc[idx,:].copy()\n",
    "saps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sapflow metadata is now isolated into its own file; Crunch a few numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['variable description', 'counter', 'sensor type', 'depth/height (m)',\n",
       "       'sap wood depth (cm)', 'species (engl.)', 'DBH (for each year)\\n[cm]',\n",
       "       'date start', 'date end', 'logger/data acquisition',\n",
       "       'level 1 data file name', 'headerout (logger)', 'headerout', 'units',\n",
       "       'headerout (final)', 'ICOS variable name', 'headerout (DB)',\n",
       "       'database ID', 'Umweltkompartiment', 'Probemedium',\n",
       "       'Physikalische Eigenschaft', 'Beobachtungsart',\n",
       "       'Zusätzliche Sensornummer', 'Zusätzliche Bezeichnung', 'Messort',\n",
       "       'Eddypro Label', 'musica', 'Min', 'Max', 'Obs_Min', 'Obs_Max',\n",
       "       'derived/influenced variables', 'compare with/emergency replacement',\n",
       "       'remarks', 'coordinates x', 'coordinates y', 'final units',\n",
       "       'maximum percentage of missing  data for flag 1 daily',\n",
       "       'maximum percentage of missing  data for flag 2 daily', 'Flag_0',\n",
       "       'Flag_1', 'Flag_2', 'Flag_3', 'Flag_4', 'Flag_5', 'Flag_6', 'Flag_7',\n",
       "       'Flag_8', 'Alert_1', 'Alert_2', 'Alert_3', 'comment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps.comment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['sensor type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Saftfluss', nan], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['Physikalische Eigenschaft'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['022_20_E', '022_20_NE', '022_20_NW', nan, '022_40_S', '026_10_E',\n",
       "       '026_10_S', '026_20_W', '026_30_N', '027_20_N', '027_20_NE',\n",
       "       '027_43_SE', '027_60_SW', '027_80_NW', '028_20_SE', '028_20_SW',\n",
       "       '028_43_W', '028_60_NE', '028_80_E', '029_20_E', '029_20_SW',\n",
       "       '029_20_N', '029_43_SE', '029_60_NW', '029_80_NE', '030_10_N',\n",
       "       '030_10_W', '030_20_S', '030_30_E', '_035_10_N', '_035_10_S',\n",
       "       '_035_20_E', '_035_30_W', '_048_20_N', '_050_10_NE', '_050_10_SW',\n",
       "       '_050_20_E', '_050_20_N', '_050_20_W', '_050_30_SE', '_050_40_NW',\n",
       "       '_050_40_W', '_056_20_N', '_057_20_NE_', '_057_20_N', '_057_20_SE',\n",
       "       '_057_43_NW', '_058_20_NW_1', '_058_20_NW_2', '_058_20_NW_3',\n",
       "       '_058_20_ENE', '_058_20_S', '_058_43_N', '068_20_NW', '068_20_S',\n",
       "       '068_43_NE', '_077_20_NE', '_077_20_SW', '_106_20_E', '_106_20_S',\n",
       "       '_106_43_N', '_108_20_ENE', '_108_20_N_1', '_108_20_N_2',\n",
       "       '_108_20_N_3', '_108_20_E', '_108_43_W', '_114_20_N',\n",
       "       '_114_20_NW_1', '_114_20_NW_2', '_114_20_NE', '_114_20_W',\n",
       "       '_114_43_SE', '_143_20_N', '_143_20_W', '_143_43_E', '_151_10_NW',\n",
       "       '_151_10_SE', '_151_20_SW', '_151_30_S', '_151_40_NE',\n",
       "       '_158_20_ENE', '_158_20_N', '_158_43_S', '_185_10_ENE',\n",
       "       '_188_20_N_1', '_188_20_N_2', '_193_20_NW_1', '_193_20_NW_2',\n",
       "       '_217_100_N', '_217_20_SE', '_217_20_SW', '_217_43_NE',\n",
       "       '_217_60_NW', '_217_80_W', '_218_20_NE_1', '_218_20_NE_2',\n",
       "       '_218_20_NE_3', '022_20_N_1', '022_20_N_2', '029_20_NE_1',\n",
       "       '029_20_NE_2', '_218_20_E', '_218_20_N', '_218_20_SE',\n",
       "       '_218_40_NW', '_282_20_N', '_282_20_NE_1', '_282_20_NE_2',\n",
       "       '_282_20_NW', '_282_43_SE', '_106_20_W_1', '_106_20_W_2',\n",
       "       '_108_20_NW_1', '_108_20_NW_2', '_143_20_NW_1', '_143_20_NW_2',\n",
       "       '_282_20_W_1', '_282_20_W_2', '_068_43_NW_N', '_080_20_E',\n",
       "       '_080_20_W', '_081_20_N', '_081_20_S', '_048_20_NE_1',\n",
       "       '_048_20_NE_2', '_057_20_NE_1', '_057_20_NE_2'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['Zusätzliche Bezeichnung'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['depth/height (m)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.04, 52.6 , 55.81, 21.3 ,   nan, 22.07, 53.51, 17.91, 62.47,\n",
       "       62.1 , 17.64, 54.39, 53.78, 53.19, 55.58, 54.9 , 54.34, 64.44,\n",
       "       45.8 , 50.6 , 46.87, 46.52, 89.22, 89.53, 89.1 , 20.06, 20.07,\n",
       "       61.5 , 60.87, 23.2 , 22.87, 33.1 , 32.74, 41.67, 42.4 , 42.1 ,\n",
       "       36.97, 36.7 , 19.6 , 20.49, 19.66, 44.25, 28.93, 34.44, 57.61,\n",
       "       54.5 , 22.14, 56.67, 55.97, 46.46, 46.54, 33.4 , 27.91, 50.31,\n",
       "       50.49, 50.22, 50.3 , 50.86, 47.82, 89.88, 62.55, 50.55, 63.7 ,\n",
       "       63.47, 18.2 , 54.88, 35.18, 38.12, 22.9 , 58.6 , 65.9 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['DBH (for each year)\\n[cm]'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'SF108_20_ENE_NT1 SF108_20_ENE_NT2',\n",
       "       'SF108_20_N_NT1_1 SF108_20_N_NT2_1',\n",
       "       'SF108_20_N_NT1_2 SF108_20_N_NT2_2'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['derived/influenced variables'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can actually be an indicator that some of the data sensors might have to be grouped with some others. The names aboove can be found in the `hederout (logger)` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tree_022', nan, 'tree_026', 'tree_027', 'tree_028', 'tree_029',\n",
       "       'tree_030', 'tree_033', 'tree_035', 'tree_048', 'tree_050',\n",
       "       'tree_056', 'tree_057', 'tree_058', 'tree_068', 'tree_077',\n",
       "       'tree_106', 'tree_108', 'tree_114', 'tree_143', 'tree_151',\n",
       "       'tree_158', 'tree_185', 'tree_188', 'tree_190', 'tree_193',\n",
       "       'tree_202', 'tree_214', 'tree_215', 'tree_217', 'tree_218',\n",
       "       'tree_233', 'tree_235', 'tree_236', 'tree_282', 'tree_301',\n",
       "       'tree_080', 'tree_081'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['Messort'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be checked against the tree numbers in the data headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([652261.0763, 652240.3695, 652247.4117, 652244.8753, 652244.2   ,\n",
       "       652247.0923, 652245.369 , 652231.8431, 652236.198 , 652221.2603,\n",
       "       652218.1973, 652208.3271, 652209.9426, 652211.8013, 652235.2166,\n",
       "       652222.7438, 652225.7665, 652218.1851, 652193.5774, 652203.252 ,\n",
       "       652205.4886, 652188.1592, 652175.4884, 652178.6211, 652237.8291,\n",
       "       652245.0562, 652247.5609, 652248.5403, 652243.469 , 652254.8028,\n",
       "       652249.5405, 652249.7757, 652251.7404, 652267.8356, 652262.2193,\n",
       "       652270.8189, 652191.1576, 652187.4286, 652232.656 , 652223.239 ,\n",
       "       652224.1483, 652249.1769, 652243.19  ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['coordinates x'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5773026.823, 5773016.98 , 5773011.187, 5773008.856, 5773010.933,\n",
       "       5773015.621, 5773019.593, 5773023.309, 5773025.685, 5773031.549,\n",
       "       5773028.326, 5773017.563, 5773013.826, 5773019.262, 5773009.919,\n",
       "       5773003.481, 5773017.072, 5773012.355, 5772994.768, 5772990.173,\n",
       "       5772975.067, 5772974.527, 5772974.008, 5772966.66 , 5772970.396,\n",
       "       5772964.414, 5772959.186, 5772970.571, 5772988.116, 5772983.157,\n",
       "       5772987.843, 5773002.173, 5772995.46 , 5772981.663, 5772989.829,\n",
       "       5772988.387, 5772978.78 , 5772994.539, 5772954.096, 5773009.32 ,\n",
       "       5773011.289, 5772969.671, 5772972.739])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['coordinates y'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not yet converted', nan, 'not present',\n",
       "       'sensors are 43mm long but installed in 20mm',\n",
       "       'hand-made sensor 20mm long', '!!!! Blank in Header !!!!! ',\n",
       "       'planned was north-east!', 'commertial sensor in 20mm depth',\n",
       "       'never installed', 'not available', 'to be checked if NT!!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['remarks'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['022', '029', '033', '048', '050', '056', '057', '058', '106',\n",
       "       '108', '114', '143', '158', '185', '188', '190', '193', '214',\n",
       "       '218', '233', '236', '282', '301'], dtype='<U3')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([col.split(' ')[0][3:6] for col in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build metadata dicts for each sapflow column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need tree number depth and direction to relate it to `Zusätzliche Bezeichnung`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [(i, col.split(' ')[0][3:]) for i, col in enumerate(df.columns) if not col.endswith('_f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['022_20_E', '022_20_NE', '022_20_NW', nan, '022_40_S', '026_10_E',\n",
       "       '026_10_S', '026_20_W', '026_30_N', '027_20_N', '027_20_NE',\n",
       "       '027_43_SE', '027_60_SW', '027_80_NW', '028_20_SE', '028_20_SW',\n",
       "       '028_43_W', '028_60_NE', '028_80_E', '029_20_E', '029_20_SW',\n",
       "       '029_20_N', '029_43_SE', '029_60_NW', '029_80_NE', '030_10_N',\n",
       "       '030_10_W', '030_20_S', '030_30_E', '_035_10_N', '_035_10_S',\n",
       "       '_035_20_E', '_035_30_W', '_048_20_N', '_050_10_NE', '_050_10_SW',\n",
       "       '_050_20_E', '_050_20_N', '_050_20_W', '_050_30_SE', '_050_40_NW',\n",
       "       '_050_40_W', '_056_20_N', '_057_20_NE_', '_057_20_N', '_057_20_SE',\n",
       "       '_057_43_NW', '_058_20_NW_1', '_058_20_NW_2', '_058_20_NW_3',\n",
       "       '_058_20_ENE', '_058_20_S', '_058_43_N', '068_20_NW', '068_20_S',\n",
       "       '068_43_NE', '_077_20_NE', '_077_20_SW', '_106_20_E', '_106_20_S',\n",
       "       '_106_43_N', '_108_20_ENE', '_108_20_N_1', '_108_20_N_2',\n",
       "       '_108_20_N_3', '_108_20_E', '_108_43_W', '_114_20_N',\n",
       "       '_114_20_NW_1', '_114_20_NW_2', '_114_20_NE', '_114_20_W',\n",
       "       '_114_43_SE', '_143_20_N', '_143_20_W', '_143_43_E', '_151_10_NW',\n",
       "       '_151_10_SE', '_151_20_SW', '_151_30_S', '_151_40_NE',\n",
       "       '_158_20_ENE', '_158_20_N', '_158_43_S', '_185_10_ENE',\n",
       "       '_188_20_N_1', '_188_20_N_2', '_193_20_NW_1', '_193_20_NW_2',\n",
       "       '_217_100_N', '_217_20_SE', '_217_20_SW', '_217_43_NE',\n",
       "       '_217_60_NW', '_217_80_W', '_218_20_NE_1', '_218_20_NE_2',\n",
       "       '_218_20_NE_3', '022_20_N_1', '022_20_N_2', '029_20_NE_1',\n",
       "       '029_20_NE_2', '_218_20_E', '_218_20_N', '_218_20_SE',\n",
       "       '_218_40_NW', '_282_20_N', '_282_20_NE_1', '_282_20_NE_2',\n",
       "       '_282_20_NW', '_282_43_SE', '_106_20_W_1', '_106_20_W_2',\n",
       "       '_108_20_NW_1', '_108_20_NW_2', '_143_20_NW_1', '_143_20_NW_2',\n",
       "       '_282_20_W_1', '_282_20_W_2', '_068_43_NW_N', '_080_20_E',\n",
       "       '_080_20_W', '_081_20_N', '_081_20_S', '_048_20_NE_1',\n",
       "       '_048_20_NE_2', '_057_20_NE_1', '_057_20_NE_2'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps['Zusätzliche Bezeichnung'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Zusätzliche Bezeichung` is somethimes prefixed with `_` and sometimes not. Strip this away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "saps['id_map'] = saps['Zusätzliche Bezeichnung'].map(lambda s: str(s).strip('_')).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which of the data headers can be mapped to the metadata using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, '022_20_E'),\n",
       " (True, '022_20_NW'),\n",
       " (True, '029_20_N'),\n",
       " (False, '033_20_NE'),\n",
       " (True, '048_20_N'),\n",
       " (True, '050_20_E'),\n",
       " (True, '050_20_N'),\n",
       " (True, '056_20_N'),\n",
       " (True, '057_20_N'),\n",
       " (True, '058_20_ENE'),\n",
       " (True, '106_20_E'),\n",
       " (True, '108_20_ENE'),\n",
       " (True, '114_20_N'),\n",
       " (True, '143_20_N'),\n",
       " (True, '158_20_ENE'),\n",
       " (True, '185_10_ENE'),\n",
       " (False, '185_10_W'),\n",
       " (False, '188_20_ENE'),\n",
       " (False, '188_20_N'),\n",
       " (False, '190_10_N'),\n",
       " (False, '193_20_E'),\n",
       " (False, '214_20_W'),\n",
       " (True, '218_20_E'),\n",
       " (True, '218_20_N'),\n",
       " (False, '233_20_N'),\n",
       " (False, '236_20_N'),\n",
       " (True, '282_20_N'),\n",
       " (False, '301_20_N'),\n",
       " (False, '301_20_W')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(col in saps.id_map.values, col) for i,col in ids] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I will only use the ones with a `True`. The other ones need more inspection as they do not align with the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some info to add the enties. We can inspect the `add_entry` api endpoint of metacatalog to see what is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add new Entry\n",
      "\n",
      "    Adds a new metadata Entry to the database. This method will create the core\n",
      "    entry. Usually, more steps are necessary, which will need the newly created\n",
      "    database ID. Such steps are:\n",
      "\n",
      "    * adding contributors   (mandatory)\n",
      "    * adding data           (extremly useful)\n",
      "    * adding keywords       (recommended)\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    session : sqlalchemy.Session\n",
      "        SQLAlchemy session connected to the database.\n",
      "    title : str\n",
      "        Title of the Entry\n",
      "    author : int, str\n",
      "        First author of the Entry. The Person record has to exist already in the\n",
      "        database and can be found by exact match on id (int) or last_name (str).\n",
      "    location : str, tuple\n",
      "        Can be either a WKT of a EPSG:4326 location, or the coordinates as a\n",
      "        tuple. It has to be (X,Y), to (longitude, latitude)\n",
      "    variable : int, str\n",
      "        **Full** variable name (str) or ID (int) of the data described by the Entry.\n",
      "    abstract : str\n",
      "        Description of the data. Be as detailed as possible\n",
      "    external_id : str\n",
      "        If the data described by Entry has another unique identifier,\n",
      "        usually supplied by the data provider, it can be stored for reference reasons.\n",
      "    comment : str\n",
      "        General purpose comment that should not contain any vital information to\n",
      "        understand the entry. If it's vital, it should go into the abstract.\n",
      "    geom : str\n",
      "        WKT of any additional geoinformation in EPSG:4326\n",
      "    license : str, int\n",
      "        Either the id or **full** name of the license to be linked to this Entry.\n",
      "    embargo : bool\n",
      "        If True, this Entry will **not** be publicly available until the embargo ends\n",
      "        The embargo period is usually 2 years but can be modified using the kwargs.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    entry: metacatalog.Entry\n",
      "        Entry instance of the added entry entity\n",
      "\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(api.add_entry.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we create\n",
    "\n",
    "* `title` - as a new string\n",
    "* `abstract` - with the variable description for now\n",
    "* `location` from coordinates_x and coordinates_y\n",
    "* `author` will be a default dummy user\n",
    "* `variable` can pass 'sap flow' or its id 14\n",
    "* `external_id` will be the rownumber from meta\n",
    "* `license` will be set to just any. has to be asked\n",
    "* `embargo` will be set to True\n",
    "\n",
    "Then we need to search for keywords, build `details`, think about contributors and finally create a datasource and upload data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['variable description', 'counter', 'sensor type', 'depth/height (m)',\n",
       "       'sap wood depth (cm)', 'species (engl.)', 'DBH (for each year)\\n[cm]',\n",
       "       'date start', 'date end', 'logger/data acquisition',\n",
       "       'level 1 data file name', 'headerout (logger)', 'headerout', 'units',\n",
       "       'headerout (final)', 'ICOS variable name', 'headerout (DB)',\n",
       "       'database ID', 'Umweltkompartiment', 'Probemedium',\n",
       "       'Physikalische Eigenschaft', 'Beobachtungsart',\n",
       "       'Zusätzliche Sensornummer', 'Zusätzliche Bezeichnung', 'Messort',\n",
       "       'Eddypro Label', 'musica', 'Min', 'Max', 'Obs_Min', 'Obs_Max',\n",
       "       'derived/influenced variables', 'compare with/emergency replacement',\n",
       "       'remarks', 'coordinates x', 'coordinates y', 'final units',\n",
       "       'maximum percentage of missing  data for flag 1 daily',\n",
       "       'maximum percentage of missing  data for flag 2 daily', 'Flag_0',\n",
       "       'Flag_1', 'Flag_2', 'Flag_3', 'Flag_4', 'Flag_5', 'Flag_6', 'Flag_7',\n",
       "       'Flag_8', 'Alert_1', 'Alert_2', 'Alert_3', 'comment', 'id_map'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saps.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'height': '1.3', 'dbh': '56.67', 'depth': '20mm', 'species': 'oak'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming this transform, the location is in the Nationalpark Hohes Holz\n",
    "# seems to make sense\n",
    "transform = pyproj.Transformer.from_crs(25832, 4326) \n",
    "\n",
    "# chunk will be iterated over ids array\n",
    "chunk = saps.where(saps.id_map == '218_20_N').dropna(how='all')\n",
    "\n",
    "core = dict(\n",
    "    title='Sap Flow - Hohes Holz - %s' % chunk.Messort.values[0].replace('_', ' ').capitalize(),\n",
    "    abstract=str(chunk['variable description'].values[0]).capitalize(),\n",
    "    location=transform.transform(chunk['coordinates x'].values[0], chunk['coordinates y'].values[0]),\n",
    "    variable=14,\n",
    "    external_id=str(chunk.index.values[0]),\n",
    "    license=6,\n",
    "    author=1,\n",
    "    embargo=True,\n",
    "    comment=str(chunk['remarks'].values[0])\n",
    ")\n",
    "\n",
    "details = dict(\n",
    "    height=str(chunk['depth/height (m)'].values[0]),\n",
    "    dbh=str(chunk['DBH (for each year)\\n[cm]'].values[0]),\n",
    "    depth=chunk.id_map.values[0].split('_')[1] + 'mm',\n",
    "    species=str(chunk['species (engl.)'].values[0])\n",
    ")\n",
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create an owner and check lookup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: Engine(postgresql://postgres:***@localhost:5432/buehlot_upload)\n"
     ]
    }
   ],
   "source": [
    "# you need to have a default connection to the DB defined\n",
    "session = api.connect_database(CONNECTION)\n",
    "print('Using: %s' % session.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Data Commons Open Database License <ID=4>\n",
      "Open Data Commons Attribution License v1.0 <ID=5>\n",
      "Creative Commons Attribution 4.0 International <ID=6>\n",
      "Creative Commons Attribution-ShareAlike 4.0 International <ID=7>\n",
      "Creative Commons Attribution-NonCommerical 4.0 International <ID=8>\n",
      "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International <ID=9>\n"
     ]
    }
   ],
   "source": [
    "for lic in api.find_license(session):\n",
    "    print(lic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I guess `ID 6` is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inst. UFZ HoH Dataholder <ID=2>\n"
     ]
    }
   ],
   "source": [
    "if UPLOAD: # turn true if you want to ADD\n",
    "    author = api.add_person(session, first_name='Inst.', last_name='UFZ HoH Dataholder')\n",
    "else:\n",
    "    author = api.find_person(session, last_name='UFZ HoH Dataholder')[0]\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load license and variable instance\n",
    "\n",
    "This should make it a bit more flexible in future, as we do not hardcode the ids anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Commons Attribution 4.0 International <ID=6>\n",
      "sap flow [cm^3/cm^2h] <ID=14>\n"
     ]
    }
   ],
   "source": [
    "license = api.find_license(session, short_title='CC BY 4.0')[0]\n",
    "variable = api.find_variable(session, name='%sap%')[0]\n",
    "\n",
    "print(license)\n",
    "print(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### go for all entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found too much metadata for 050_20_N\n"
     ]
    }
   ],
   "source": [
    "# assuming this transform, the location is in the Nationalpark Hohes Holz\n",
    "# seems to make sense\n",
    "transform = pyproj.Transformer.from_crs(25832, 4326, always_xy=True) \n",
    "\n",
    "meta_json = dict()\n",
    "\n",
    "for i, col_name in ids:\n",
    "    if not col_name in saps.id_map.values:\n",
    "        continue\n",
    "    chunk = saps.where(saps.id_map==col_name).dropna(how='all')\n",
    "    if len(chunk.index) > 1:\n",
    "        print('Found too much metadata for %s' % col_name)\n",
    "        continue\n",
    "    \n",
    "    core = dict(\n",
    "        title='Sap Flow - Hohes Holz - %s' % chunk.Messort.values[0].replace('_', ' ').capitalize(),\n",
    "        abstract=str(chunk['variable description'].values[0]).capitalize(),\n",
    "        location=transform.transform(chunk['coordinates x'].values[0], chunk['coordinates y'].values[0]),\n",
    "        variable=variable.id,\n",
    "        external_id=str(chunk.index.values[0]),\n",
    "        license=license.id,\n",
    "        author=author.id,\n",
    "        embargo=True,\n",
    "        comment=str(chunk['remarks'].values[0])\n",
    "    )\n",
    "    details = dict(\n",
    "        height=str(chunk['depth/height (m)'].values[0]),\n",
    "        dbh=str(chunk['DBH (for each year)\\n[cm]'].values[0]),\n",
    "        depth=chunk.id_map.values[0].split('_')[1] + 'mm',\n",
    "        species=str(chunk['species (engl.)'].values[0])\n",
    "    )\n",
    "    \n",
    "    # add to meta_json\n",
    "    meta_json[i] = dict(core=core, details=details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, there are at least two metadata entries for tree 50, depth 20mm and north orientation. We have to check that\n",
    "\n",
    "\n",
    "The JSON top level key identifies the data column for import. `core` are the mandatory meta data for `Entry`, details are the additional details to describe sapflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'core': {'abstract': 'Sap flow tree 022- 20mm- east',\n",
      "              'author': 2,\n",
      "              'comment': 'not yet converted',\n",
      "              'embargo': True,\n",
      "              'external_id': '212',\n",
      "              'license': 6,\n",
      "              'location': (11.222353532314015, 52.08683783324311),\n",
      "              'title': 'Sap Flow - Hohes Holz - Tree 022',\n",
      "              'variable': 14},\n",
      "     'details': {'dbh': '53.04',\n",
      "                 'depth': '20mm',\n",
      "                 'height': '1.3',\n",
      "                 'species': 'oak'}},\n",
      " 2: {'core': {'abstract': 'Sap flow tree 022- 20mm- north-west',\n",
      "              'author': 2,\n",
      "              'comment': 'not yet converted',\n",
      "              'embargo': True,\n",
      "              'external_id': '214',\n",
      "              'license': 6,\n",
      "              'location': (11.222353532314015, 52.08683783324311),\n",
      "              'title': 'Sap Flow - Hohes Holz - Tree 022',\n",
      "              'variable': 14},\n",
      "     'details': {'dbh': '53.04',\n",
      "                 'depth': '20mm',\n",
      "                 'height': '1.3',\n",
      "                 'species': 'oak'}},\n",
      " 4: {'core': {'abstract': 'Sap flow tree 029- 20mm- north',\n",
      "              'author': 2,\n",
      "              'comment': 'not yet converted',\n",
      "              'embargo': True,\n",
      "              'external_id': '246',\n",
      "              'license': 6,\n",
      "              'location': (11.222144591224945, 52.08674104008754),\n",
      "              'title': 'Sap Flow - Hohes Holz - Tree 029',\n",
      "              'variable': 14},\n",
      "     'details': {'dbh': '22.07',\n",
      "                 'depth': '20mm',\n",
      "                 'height': '1.3',\n",
      "                 'species': 'hornbeam'}},\n",
      " 8: {'core': {'abstract': 'Sap flow tree 048- 20mm- north 2015',\n",
      "              'author': 2,\n",
      "              'comment': 'nan',\n",
      "              'embargo': True,\n",
      "              'external_id': '269',\n",
      "              'license': 6,\n",
      "              'location': (11.221774973337356, 52.0868912422611),\n",
      "              'title': 'Sap Flow - Hohes Holz - Tree 048',\n",
      "              'variable': 14},\n",
      "     'details': {'dbh': '17.91',\n",
      "                 'depth': '20mm',\n",
      "                 'height': '1.3',\n",
      "                 'species': 'hornbeam'}},\n",
      " 10: {'core': {'abstract': 'Sap flow tree 050- 20mm- east',\n",
      "               'author': 2,\n",
      "               'comment': 'nan',\n",
      "               'embargo': True,\n",
      "               'external_id': '275',\n",
      "               'license': 6,\n",
      "               'location': (11.221728864418242, 52.086863128916136),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 050',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '62.47',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'oak'}},\n",
      " 14: {'core': {'abstract': 'Sap flow tree 056- 20mm- north',\n",
      "               'author': 2,\n",
      "               'comment': 'nan',\n",
      "               'embargo': True,\n",
      "               'external_id': '282',\n",
      "               'license': 6,\n",
      "               'location': (11.221580115230608, 52.086769147511106),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 056',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '17.64',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'hornbeam'}},\n",
      " 16: {'core': {'abstract': 'Sap flow tree 057- 20mm- north',\n",
      "               'author': 2,\n",
      "               'comment': 'nan',\n",
      "               'embargo': True,\n",
      "               'external_id': '292',\n",
      "               'license': 6,\n",
      "               'location': (11.221602007312564, 52.08673512986815),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 057',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '53.78',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'beech'}},\n",
      " 18: {'core': {'abstract': 'Sap flow tree 058- 20mm- east-north-east',\n",
      "               'author': 2,\n",
      "               'comment': 'nan',\n",
      "               'embargo': True,\n",
      "               'external_id': '306',\n",
      "               'license': 6,\n",
      "               'location': (11.221631540563834, 52.08678345614037),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 058',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': 'nan',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'beech'}},\n",
      " 20: {'core': {'abstract': 'Sap flow tree 106 - 20mm - east - commertial',\n",
      "               'author': 2,\n",
      "               'comment': 'commertial sensor in 20mm depth',\n",
      "               'embargo': True,\n",
      "               'external_id': '326',\n",
      "               'license': 6,\n",
      "               'location': (11.221354835361566, 52.086568411238765),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 106',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '46.87',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'beech'}},\n",
      " 22: {'core': {'abstract': 'Sap flow tree 108 - 20mm - east-north-east - '\n",
      "                           'commertial',\n",
      "               'author': 2,\n",
      "               'comment': 'commertial sensor in 20mm depth',\n",
      "               'embargo': True,\n",
      "               'external_id': '333',\n",
      "               'license': 6,\n",
      "               'location': (11.22149387611781, 52.08652446950051),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 108',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '89.22',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'beech'}},\n",
      " 24: {'core': {'abstract': 'Sap flow tree 114 - 20mm - north - commertial',\n",
      "               'author': 2,\n",
      "               'comment': 'commertial sensor in 20mm depth',\n",
      "               'embargo': True,\n",
      "               'external_id': '347',\n",
      "               'license': 6,\n",
      "               'location': (11.221519751750522, 52.08638814129585),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 114',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '20.06',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'hornbeam'}},\n",
      " 26: {'core': {'abstract': 'Sap flow tree 143 - 20mm - north - commertial',\n",
      "               'author': 2,\n",
      "               'comment': 'commertial sensor in 20mm depth',\n",
      "               'embargo': True,\n",
      "               'external_id': '361',\n",
      "               'license': 6,\n",
      "               'location': (11.221266784688458, 52.08638805443536),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 143',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '61.5',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'beech'}},\n",
      " 28: {'core': {'abstract': 'Sap flow tree 158 - 20mm - east-north-east - '\n",
      "                           'commertial',\n",
      "               'author': 2,\n",
      "               'comment': 'commertial sensor in 20mm depth',\n",
      "               'embargo': True,\n",
      "               'external_id': '373',\n",
      "               'license': 6,\n",
      "               'location': (11.221124173623972, 52.086319998966204),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 158',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '23.2',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'beech'}},\n",
      " 30: {'core': {'abstract': 'Sap flow tree 185 - 10mm - ene',\n",
      "               'author': 2,\n",
      "               'comment': 'nan',\n",
      "               'embargo': True,\n",
      "               'external_id': '380',\n",
      "               'license': 6,\n",
      "               'location': (11.22198930916518, 52.08633728363027),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 185',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '33.1',\n",
      "                  'depth': '10mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'oak'}},\n",
      " 44: {'core': {'abstract': 'Sap flow tree 218- 20mm- east',\n",
      "               'author': 2,\n",
      "               'comment': 'not yet converted',\n",
      "               'embargo': True,\n",
      "               'external_id': '447',\n",
      "               'license': 6,\n",
      "               'location': (11.222203376714393, 52.086558634098814),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 218',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '56.67',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'oak'}},\n",
      " 46: {'core': {'abstract': 'Sap flow tree 218- 20mm- north',\n",
      "               'author': 2,\n",
      "               'comment': 'not yet converted',\n",
      "               'embargo': True,\n",
      "               'external_id': '448',\n",
      "               'license': 6,\n",
      "               'location': (11.222203376714393, 52.086558634098814),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 218',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '56.67',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'oak'}},\n",
      " 52: {'core': {'abstract': 'Sap flow tree 282 - 20mm - north - commertial',\n",
      "               'author': 2,\n",
      "               'comment': 'commertial sensor in 20mm depth',\n",
      "               'embargo': True,\n",
      "               'external_id': '465',\n",
      "               'license': 6,\n",
      "               'location': (11.221312410395242, 52.08642543931798),\n",
      "               'title': 'Sap Flow - Hohes Holz - Tree 282',\n",
      "               'variable': 14},\n",
      "      'details': {'dbh': '50.31',\n",
      "                  'depth': '20mm',\n",
      "                  'height': '1.3',\n",
      "                  'species': 'beech'}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "with open('metadata.json', 'w') as js:\n",
    "    json.dump(meta_json, js, indent=4)\n",
    "    \n",
    "pprint(meta_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded a total of 17 Entry\n"
     ]
    }
   ],
   "source": [
    "if UPLOAD:\n",
    "    entries = []\n",
    "\n",
    "    for k,m in meta_json.items():\n",
    "        core = m['core']\n",
    "        details = m['details']\n",
    "        \n",
    "        # create core entry\n",
    "        e = api.add_entry(session, **core)\n",
    "        \n",
    "        # add details\n",
    "        api.add_details_to_entries(session, e, **details)\n",
    "\n",
    "        meta_json[k]['entry_id'] = e.id\n",
    "        entries.append(e)\n",
    "else:\n",
    "    entries = api.find_entry(session)\n",
    "    for m, e in zip(meta_json.items(), entries):\n",
    "        k, d = m\n",
    "        if d['core']['external_id'] == e.external_id:\n",
    "            meta_json[k]['entry_id'] = e.id\n",
    "            \n",
    "with open('metadata.json', 'w') as js:\n",
    "    json.dump(meta_json, js, indent=4)\n",
    "\n",
    "print('%s a total of %d Entry' % ('Uploaded' if UPLOAD else 'Loaded', len(entries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data-sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the default `timeseries` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPLOAD:\n",
    "    for e in entries:\n",
    "        e.create_datasource(type=1, path='timeseries', datatype='timeseries' ,commit=True)\n",
    "else:\n",
    "    print('All Entry have a datasource: ', all([e.datasource is not None for e in entries]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data transform\n",
    "\n",
    "Do it at the example of entry of ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EID = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index in meta_json:  16\n"
     ]
    }
   ],
   "source": [
    "e = entries[EID]\n",
    "\n",
    "# this is how you find the correct column\n",
    "meta_json_id = [k for k,d in meta_json.items() if d['entry_id']==e.id][0]\n",
    "print('Index in meta_json: ', meta_json_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SFD057_20_N [cm**3/(cm**2 10min**1)]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,meta_json_id].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sap Flow - Hohes Holz - Tree 057\n",
      "|        |      id | key     | stem   | value   |   entry_id | entry_uuid                           |\n",
      "|:-------|--------:|:--------|:-------|:--------|-----------:|:-------------------------------------|\n",
      "| height | 2299366 | height  | height | 1.3     |         51 | daf4d076-28ed-4de4-9f1c-f0ae46a76393 |\n",
      "| dbh    | 2299367 | dbh     | dbh    | 53.78   |         51 | daf4d076-28ed-4de4-9f1c-f0ae46a76393 |\n",
      "| depth  | 2299368 | depth   | depth  | 20mm    |         51 | daf4d076-28ed-4de4-9f1c-f0ae46a76393 |\n",
      "| speci  | 2299369 | species | speci  | beech   |         51 | daf4d076-28ed-4de4-9f1c-f0ae46a76393 |\n"
     ]
    }
   ],
   "source": [
    "print(e.title)\n",
    "print(e.details_table(fmt='markdown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked fine, it's the same tree. We need to tweak the details to also include the direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Date Time'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,meta_json_id].plot(figsize=(18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. This is not useful value. Check the others as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SFD022_20_E [cm**3/(cm**2 10min**1)]      5.253951e+00\n",
       "SFD022_20_NW [cm**3/(cm**2 10min**1)]     3.071833e+00\n",
       "SFD029_20_N [cm**3/(cm**2 10min**1)]      3.770771e+04\n",
       "SFD033_20_NE [cm**3/(cm**2 10min**1)]     4.865212e+06\n",
       "SFD048_20_N [cm**3/(cm**2 10min**1)]      1.817722e+21\n",
       "SFD050_20_E [cm**3/(cm**2 10min**1)]      3.290917e+00\n",
       "SFD050_20_N [cm**3/(cm**2 10min**1)]      1.272950e+00\n",
       "SFD056_20_N [cm**3/(cm**2 10min**1)]      1.963521e+02\n",
       "SFD057_20_N [cm**3/(cm**2 10min**1)]      8.335309e+02\n",
       "SFD058_20_ENE [cm**3/(cm**2 10min**1)]    9.942780e+02\n",
       "SFD106_20_E [cm**3/(cm**2 10min**1)]      5.220397e+02\n",
       "SFD108_20_ENE [cm**3/(cm**2 10min**1)]    3.571688e+06\n",
       "SFD114_20_N [cm**3/(cm**2 10min**1)]      8.539200e+07\n",
       "SFD143_20_N [cm**3/(cm**2 10min**1)]      1.504714e+06\n",
       "SFD158_20_ENE [cm**3/(cm**2 10min**1)]    1.873601e+07\n",
       "SFD185_10_ENE [cm**3/(cm**2 10min**1)]    2.157753e+00\n",
       "SFD185_10_W [cm**3/(cm**2 10min**1)]      3.300940e+00\n",
       "SFD188_20_ENE [cm**3/(cm**2 10min**1)]    1.732202e+00\n",
       "SFD188_20_N [cm**3/(cm**2 10min**1)]      1.223844e+00\n",
       "SFD190_10_N [cm**3/(cm**2 10min**1)]      3.846106e+00\n",
       "SFD193_20_E [cm**3/(cm**2 10min**1)]      2.630465e+00\n",
       "SFD214_20_W [cm**3/(cm**2 10min**1)]      6.210580e+05\n",
       "SFD218_20_E [cm**3/(cm**2 10min**1)]      4.298181e+00\n",
       "SFD218_20_N [cm**3/(cm**2 10min**1)]      7.328380e+00\n",
       "SFD233_20_N [cm**3/(cm**2 10min**1)]      3.514752e+00\n",
       "SFD236_20_N [cm**3/(cm**2 10min**1)]      5.089177e+01\n",
       "SFD282_20_N [cm**3/(cm**2 10min**1)]      4.419083e+04\n",
       "SFD301_20_N [cm**3/(cm**2 10min**1)]      4.365238e+00\n",
       "SFD301_20_W [cm**3/(cm**2 10min**1)]      2.586268e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,list(range(0,58,2))].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be related to the flags. Remove everything that is lager than 900 (so no 1 or 2 in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833.530914163\n",
      "4.74750610396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Date Time'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor = df.iloc[:, [meta_json_id,meta_json_id + 1]]\n",
    "sensor.columns = ['data', 'flag']\n",
    "print(sensor.data.max())\n",
    "print(sensor.where(sensor.flag <= 900.).dropna().data.max())\n",
    "sensor.where(sensor.flag <= 900.).dropna().plot(figsize=(16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 4, 22, 17, 30)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor.dropna().index[0].to_pydatetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum value is 4.7 now. That looks WAY better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ID=45  loaded 22160 points\n",
      "done ID=46  loaded 15121 points\n",
      "done ID=47  loaded 25414 points\n",
      "done ID=48  loaded 27043 points\n",
      "done ID=49  loaded 16976 points\n",
      "done ID=50  loaded 21252 points\n",
      "done ID=51  loaded 27973 points\n",
      "done ID=52  loaded 25323 points\n",
      "done ID=53  loaded 24668 points\n",
      "done ID=54  loaded 28156 points\n",
      "done ID=55  loaded 28188 points\n",
      "done ID=56  loaded 26823 points\n",
      "done ID=57  loaded 28176 points\n",
      "done ID=58  loaded 26304 points\n",
      "done ID=59  loaded 23842 points\n",
      "done ID=60  loaded 21265 points\n",
      "done ID=61  loaded 28182 points\n"
     ]
    }
   ],
   "source": [
    "if UPLOAD or True:   # if you don't want to upload entries but data, set to True\n",
    "    for e in entries:\n",
    "        i = [k for k,d in meta_json.items() if d['entry_id']==e.id][0]\n",
    "#        if e.id <= 12:\n",
    "#            print('skipping ID=%d' % e.id)\n",
    "#            continue\n",
    "\n",
    "        dat = df.iloc[:, [i,i+1]]\n",
    "        dat.columns = ['data', 'flag']\n",
    "        sensor = dat.where(dat.flag <= 900.).dropna()\n",
    "\n",
    "        # entry_id\n",
    "        sensor.index.name = 'tstamp'\n",
    "        imp = pd.DataFrame(sensor.iloc[:,0]).dropna()\n",
    "        imp.columns = ['value']\n",
    "\n",
    "        e.import_data(imp)\n",
    "        \n",
    "        # add temporal scale\n",
    "        e.datasource.create_scale(\n",
    "            resolution='10min', \n",
    "            extent=(imp.index[0].to_pydatetime(), imp.index[-1].to_pydatetime()), \n",
    "            support=1.0, \n",
    "            scale_dimension='temporal'\n",
    "        )\n",
    "        session.add(e)\n",
    "        session.commit()\n",
    "        print('done ID=%d  loaded %d points' % (e.id, len(imp)))\n",
    "else:\n",
    "    print('No data was uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ID=16 Bühlot dataset: Rain [air temperature] >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='tstamp'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = api.connect_database(CONNECTION)\n",
    "entry = api.find_entry(session, id=meta_json_id)[0]\n",
    "print(entry)\n",
    "data = entry.get_data()\n",
    "\n",
    "data.plot(figsize=(16,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these chunks to migrate data that was uploaded prior to metacatalog 0.2. It will run two adaptions:\n",
    "\n",
    "1. Update the user\n",
    "2. Add the temporal scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b7bbd7e81a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONNECTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mauthor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_person\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganisation_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'*UFZ*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s %s has %d Entries'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "session = api.connect_database(CONNECTION)\n",
    "\n",
    "author = api.find_person(session, organisation_name='*UFZ*')[0]\n",
    "print(author)\n",
    "print('%s %s has %d Entries' % (author.first_name, author.last_name, len(author.entries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to change the `UFZ HoH Dataholder` as `metacatalog>=0.2` needs the first author to be a real person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MIGRATE:\n",
    "    author.first_name = \n",
    "    author.last_name = \n",
    "    author.organisation_name = 'UFZ Leipzig'\n",
    "    author.affiliation = \n",
    "    author.attribution = \n",
    "\n",
    "    session.add(author)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datatype\n",
    "\n",
    "We are also missing the datatype name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the datatype\n",
    "datatype= session.query(models.DataType).filter(models.DataType.name=='timeseries').one()\n",
    "\n",
    "for asoc in author.entries:\n",
    "    e = asoc.entry\n",
    "    if e.datasource.datatype is None:\n",
    "        e.datasource.datatype = datatype\n",
    "        session.add(e)\n",
    "        session.commit()\n",
    "        print('Entry [%d] added DataType timeseries' % (e.id))\n",
    "    elif e.datasource.datatype.name  != 'timeseries':\n",
    "        e.datasource.datatype = datatype\n",
    "        session.add(e)\n",
    "        session.commit()\n",
    "        print('Entry [%d] changed to timeseries' % (e.id))\n",
    "    else:\n",
    "        print('Entry [%d] has DataType timeseries' % (e.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a temporal scale and set the citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MIGRATE:\n",
    "    for asoc in author.entries:\n",
    "        e = asoc.entry\n",
    "        if e.datasource.temporal_scale is not None:\n",
    "            print('Entry [%d] already has a temporal scale' % (e.id))\n",
    "        else:\n",
    "            data = e.get_data()\n",
    "            e.datasource.create_scale(\n",
    "                resolution='10min', \n",
    "                extent=(data.index[0].to_pydatetime(), data.index[-1].to_pydatetime()), \n",
    "                support=1.0, \n",
    "                scale_dimension='temporal'\n",
    "            )\n",
    "            session.add(e)\n",
    "            session.commit()\n",
    "            print('Entry [%d] added scale' % (e.id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### citation \n",
    "\n",
    "We need citation information information for each of the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = author.entries[0].entry\n",
    "\n",
    "# authors - title - org - year\n",
    "tpl = \"\"\"%s: %s, %s, %s, %s.\"\"\"\n",
    "\n",
    "au = \"%s, %s.\" % (e.author.last_name, e.author.first_name[:1]) # add contributors if any\n",
    "\n",
    "citation = tpl % (au, e.title, 'V-FOR-WaTer portal', 'PERMANENT URI', e.publication.year)\n",
    "print(citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for asoc in author.entries:\n",
    "    e = asoc.entry\n",
    "    e.citation = tpl % ('%s, %s.' % (e.author.last_name, e.author.first_name[:1]), e.title, 'V-FOR-WaTer portal', 'PERMANENT URI', e.publication.year)\n",
    "\n",
    "session.add(author)\n",
    "session.commit()"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1595230514539,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
