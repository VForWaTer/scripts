{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6600a093",
   "metadata": {},
   "source": [
    "# Bühlot data preprocessing\n",
    "\n",
    "The purpose of this code is to read in all the collected data, sort it by their different variables and then safe it in the correct folder.\n",
    "By running this code ALL the collected data will be processed, not just the new data. Therefore all the previous sorted data will be overwritten. The sorted data will be safed in a folder named \"data_export\".\n",
    "\n",
    "This is a list of all the variables:\n",
    "- air temperature [°C]\n",
    "- bulk electrical conductivity [dS/m]\n",
    "- ground water level [mm]\n",
    "- logger temperature [°C]\n",
    "- precipitation [mm]\n",
    "- river water level 1 []\n",
    "- river water level 2 []\n",
    "- river water level 4 []\n",
    "- volumetric water content [m^3/m^3]\n",
    "- water temperature [°C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73310cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if it does not exist\n",
    "os.makedirs(\"data/data_export/air_temperature\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/bulk_electrical_conductivity\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/ground_water_level\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/logger_temperature\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/precipitation\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/river_water_level_1\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/river_water_level_2\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/river_water_level_4\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/volumetric_water_content\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/water_temperature\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758b3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filename, variable):\n",
    "    \"\"\"\n",
    "    This function preprocesses the raw data files for the needed variable.\n",
    "    It will seperate a data file into the different variables.\n",
    "    It reads in the raw data to then create a tabel with the columns that are needed. \n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    if variable == 'precipitation':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, skiprows=1, na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'precipitation']\n",
    "        \n",
    "        # convert to datetime\n",
    "        try: \n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            \n",
    "        except ValueError:\n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%y %H:%M:%S')\n",
    "                   \n",
    "        # drop from df where precipitation is NaN\n",
    "        df.dropna(subset=[\"precipitation\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'air temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, skiprows=1, na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "        \n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'air_temperature']\n",
    "        \n",
    "        # convert to datetime      \n",
    "        try: \n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            \n",
    "        except ValueError:\n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%y %H:%M:%S')\n",
    "            \n",
    "        # drop from df where ait temperature is NaN\n",
    "        df.dropna(subset=[\"air_temperature\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'Table1_VWC':\n",
    "        \n",
    "        # read in raw data from table 1\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'volumetric_water_content']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # drop from df where voulemtric water content is NaN\n",
    "        df.dropna(subset=[\"volumetric_water_content\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'Table1_EC':\n",
    "        \n",
    "        # read in raw data from table 1\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'bulk_electrical_conductivity']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # drop from df where bulk electrical conductivity is NaN\n",
    "        df.dropna(subset=[\"bulk_electrical_conductivity\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'Table2_VWC':\n",
    "        \n",
    "        # read in raw data from table 2\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'volumetric_water_content']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # drop from df where voulemtric water content is NaN\n",
    "        df.dropna(subset=[\"volumetric_water_content\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'Table2_EC':\n",
    "        \n",
    "        # read in raw data from table 2\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'bulk_electrical_conductivity']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # drop from df where bulk electrical conductivity is NaN\n",
    "        df.dropna(subset=[\"bulk_electrical_conductivity\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'ground water level':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_height']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # drop from df where water height is NaN\n",
    "        df.dropna(subset=[\"water_height\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'water temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # drop from df where water temperature is NaN\n",
    "        df.dropna(subset=[\"water_temperature\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'logger temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'logger_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # drop from df where logger temperature is NaN\n",
    "        df.dropna(subset=[\"logger_temperature\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'ground water level csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_height']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        # drop from df where water height is NaN\n",
    "        df.dropna(subset=[\"water_height\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'water temperature csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        # drop from df where water temperature is NaN\n",
    "        df.dropna(subset=[\"water_temperature\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'logger temperature csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'logger_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        # drop from df where logger temperature is NaN\n",
    "        df.dropna(subset=[\"logger_temperature\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'river water level 1':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river_water_level_1', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river_water_level_1']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "        # drop from df where river water level 1 is NaN\n",
    "        df.dropna(subset=[\"river_water_level_1\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'river water level 2':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river_water_level_2', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river_water_level_2']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "        # drop from df where river water level 2 is NaN\n",
    "        df.dropna(subset=[\"river_water_level_2\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'river water level 4':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river_water_level_4', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river_water_level_4']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "        # drop from df where river water level 4 is NaN\n",
    "        df.dropna(subset=[\"river_water_level_4\"], inplace=True)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Variable is '{variable}', must be in ['precipitation', 'air temperature', 'Table1_VWC', 'Table1_EC', 'Table2_VWC', 'Table2_EC', 'ground water level', 'water temperature', 'logger temperature', 'ground water level csv', 'water temperature csv', 'logger temperature csv', 'river water level 1', 'river water level 2', 'river water level 4']\")\n",
    "    \n",
    "    # return preprocessed dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3751058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(variable):\n",
    "    \"\"\"\n",
    "    This function merges all the data for the assigned list. \n",
    "    It will rename the file in a correct way (name_variable) and then safe it in the associated folder. \n",
    "    Duplicates are not excepted, so they must be removed from each file.\n",
    "    \n",
    "    \"Table1\" and \"Table2\" are names from the data file volumetric water content. Each station has two sensors (\"Table1\" and \"Table2\"). \n",
    "    While the sensor from \"Table1\" is placed in a depth of 20 cm below the top edge of the ground, the other sensor \"Table2\" is placed in a \n",
    "    depth of 50 cm below the top edge of the ground.\n",
    "    \n",
    "    The abbreviations are:\n",
    "    AT = air temperature\n",
    "    P = precipitation\n",
    "    VWC_1 = volumetric water content of \"Table1\"\n",
    "    EC_1 = bulk electrical conductivity of \"Table1\"\n",
    "    VWC_2 = volumetric water content of \"Table2\"\n",
    "    EC_2 = bulk electrical conductivity of \"Table2\"\n",
    "    GWL = ground water level\n",
    "    WT = water temperature\n",
    "    LT = logger temperature\n",
    "    GWL_csv = ground water level\n",
    "    WT_csv = water temperature\n",
    "    LG_csv = logger temperature\n",
    "    RWL_1 = river water level from the first sensor\n",
    "    RWL_2 = river water level from the second sensor\n",
    "    RWL_4 = river water level from the third sensor - sensor is named with number 4 \n",
    "    \n",
    "    \"\"\"\n",
    "       \n",
    "    if variable == 'all_data_AT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_AT = pd.concat(all_data_AT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_AT.sort_index(axis='index', inplace=False)\n",
    "                        \n",
    "        # drop duplicates\n",
    "        df_all_data_AT = df_all_data_AT.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename \n",
    "        filename_AT = filename.replace(\".csv\", \"_air_temperature.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_AT['tstamp'])==len(set(df_all_data_AT['tstamp']))\n",
    "            df_all_data_AT.to_csv(f'data/data_export/air_temperature/{filename_AT}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_AT} sind duplikate vorhanden.')\n",
    "            df_all_data_AT.to_csv(f'data/data_duplicates/{filename_AT}', index=False)\n",
    "                   \n",
    "    elif variable == 'all_data_P':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_P = pd.concat(all_data_P, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_P.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_P = df_all_data_P.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_P = filename.replace(\".csv\", \"_precipitation.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_P['tstamp'])==len(set(df_all_data_P['tstamp']))\n",
    "            df_all_data_P.to_csv(f'data/data_export/precipitation/{filename_P}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_P} sind duplikate vorhanden.')\n",
    "            df_all_data_P.to_csv(f'data/data_duplicates/{filename_P}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_VWC_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_VWC_1 = pd.concat(all_data_VWC_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_VWC_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_VWC_1 = df_all_data_VWC_1.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_VWC_1 = filename.replace(\"_Table1.dat\", \"_volumetric_water_content_20cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_VWC_1['tstamp'])==len(set(df_all_data_VWC_1['tstamp']))\n",
    "            df_all_data_VWC_1.to_csv(f'data/data_export/volumetric_water_content/{filename_VWC_1}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_VWC_1} sind duplikate vorhanden.')\n",
    "            df_all_data_VWC_1.to_csv(f'data/data_duplicates/{filename_VWC_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_EC_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_EC_1 = pd.concat(all_data_EC_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_EC_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_EC_1 = df_all_data_EC_1.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_EC_1 = filename.replace(\"_Table1.dat\", \"_bulk_electrical_conductivity_20cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_EC_1['tstamp'])==len(set(df_all_data_EC_1['tstamp']))\n",
    "            df_all_data_EC_1.to_csv(f'data/data_export/bulk_electrical_conductivity/{filename_EC_1}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_EC_1} sind duplikate vorhanden.')\n",
    "            df_all_data_EC_1.to_csv(f'data/data_duplicates/{filename_EC_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_VWC_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_VWC_2 = pd.concat(all_data_VWC_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_VWC_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_VWC_2 = df_all_data_VWC_2.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_VWC_2 = filename.replace(\"_Table2.dat\", \"_volumetric_water_content_50cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_VWC_2['tstamp'])==len(set(df_all_data_VWC_2['tstamp']))\n",
    "            df_all_data_VWC_2.to_csv(f'data/data_export/volumetric_water_content/{filename_VWC_2}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_VWC_2} sind duplikate vorhanden.')\n",
    "            df_all_data_VWC_2.to_csv(f'data/data_duplicates/{filename_VWC_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_EC_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_EC_2 = pd.concat(all_data_EC_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_EC_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_EC_2 = df_all_data_EC_2.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_EC_2 = filename.replace(\"_Table2.dat\", \"_bulk_electrical_conductivity_50cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_EC_2['tstamp'])==len(set(df_all_data_EC_2['tstamp']))\n",
    "            df_all_data_EC_2.to_csv(f'data/data_export/bulk_electrical_conductivity/{filename_EC_2}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_EC_2} sind duplikate vorhanden.')\n",
    "            df_all_data_EC_2.to_csv(f'data/data_duplicates/{filename_EC_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_GWL':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_GWL = pd.concat(all_data_GWL, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_GWL.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_GWL = df_all_data_GWL.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_GWL = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_ground_water_level.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_GWL['tstamp'])==len(set(df_all_data_GWL['tstamp']))\n",
    "            df_all_data_GWL.to_csv(f'data/data_export/ground_water_level/{filename_GWL}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_GWL} sind duplikate vorhanden.')\n",
    "            df_all_data_GWL.to_csv(f'data/data_duplicates/{filename_GWL}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_WT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_WT = pd.concat(all_data_WT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_WT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_WT = df_all_data_WT.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_WT = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_water_temperature.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_WT['tstamp'])==len(set(df_all_data_WT['tstamp']))\n",
    "            df_all_data_WT.to_csv(f'data/data_export/water_temperature/{filename_WT}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_WT} sind duplikate vorhanden.')\n",
    "            df_all_data_WT.to_csv(f'data/data_duplicates/{filename_WT}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_LT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_LT = pd.concat(all_data_LT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_LT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_LT = df_all_data_LT.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_LT = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_logger_temperature.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_LT['tstamp'])==len(set(df_all_data_LT['tstamp']))\n",
    "            df_all_data_LT.to_csv(f'data/data_export/logger_temperature/{filename_LT}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_LT} sind duplikate vorhanden.')\n",
    "            df_all_data_LT.to_csv(f'data/data_duplicates/{filename_LT}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_GWL_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_GWL_csv = pd.concat(all_data_GWL_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_GWL_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_GWL_csv = df_all_data_GWL_csv.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_GWL_csv = filename.replace(\".csv\", \"_GWL.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_GWL_csv['tstamp'])==len(set(df_all_data_GWL_csv['tstamp']))\n",
    "            df_all_data_GWL_csv.to_csv(f'data/data_export/ground_water_level/{filename_GWL_csv}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_GWL_csv} sind duplikate vorhanden.')\n",
    "            df_all_data_GWL_csv.to_csv(f'data/data_duplicates/{filename_GWL_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_WT_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_WT_csv = pd.concat(all_data_WT_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_WT_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_WT_csv = df_all_data_WT_csv.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_WT_csv = filename.replace(\".csv\", \"_WT.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_WT_csv['tstamp'])==len(set(df_all_data_WT_csv['tstamp']))\n",
    "            df_all_data_WT_csv.to_csv(f'data/data_export/water_temperature/{filename_WT_csv}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_WT_csv} sind duplikate vorhanden.')\n",
    "            df_all_data_WT_csv.to_csv(f'data/data_duplicates/{filename_WT_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_LT_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_LT_csv = pd.concat(all_data_LT_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_LT_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_LT_csv = df_all_data_LT_csv.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_LT_csv = filename.replace(\".csv\", \"_LT.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_LT_csv['tstamp'])==len(set(df_all_data_LT_csv['tstamp']))\n",
    "            df_all_data_LT_csv.to_csv(f'data/data_export/logger_temperature/{filename_LT_csv}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_LT_csv} sind duplikate vorhanden.')\n",
    "            df_all_data_LT_csv.to_csv(f'data/data_duplicates/{filename_LT_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_1 = pd.concat(all_data_RWL_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_RWL_1 = df_all_data_RWL_1.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_1 = filename.replace(\"Pegel1_\", \"\").replace(\".csv\", \"_river_water_level_1.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_RWL_1['tstamp'])==len(set(df_all_data_RWL_1['tstamp']))\n",
    "            df_all_data_RWL_1.to_csv(f'data/data_export/river_water_level_1/{filename_RWL_1}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_RWL_1} sind duplikate vorhanden.')\n",
    "            df_all_data_RWL_1.to_csv(f'data/data_duplicates/{filename_RWL_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_2 = pd.concat(all_data_RWL_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_RWL_2 = df_all_data_RWL_2.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_2 = filename.replace(\"Pegel2_\", \"\").replace(\".csv\", \"_river_water_level_2.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_RWL_2['tstamp'])==len(set(df_all_data_RWL_2['tstamp']))\n",
    "            df_all_data_RWL_2.to_csv(f'data/data_export/river_water_level_2/{filename_RWL_2}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_RWL_2} sind duplikate vorhanden.')\n",
    "            df_all_data_RWL_2.to_csv(f'data/data_duplicates/{filename_RWL_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_4':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_4 = pd.concat(all_data_RWL_4, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_4.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # drop duplicates\n",
    "        df_all_data_RWL_4 = df_all_data_RWL_4.drop_duplicates(subset=['tstamp'], keep='first', ignore_index=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_4 = filename.replace(\"Pegel4_\", \"\").replace(\".csv\", \"_river_water_level_4.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_RWL_4['tstamp'])==len(set(df_all_data_RWL_4['tstamp']))\n",
    "            df_all_data_RWL_4.to_csv(f'data/data_export/river_water_level_4/{filename_RWL_4}', index=False)\n",
    "            \n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_RWL_4} sind duplikate vorhanden.')\n",
    "            df_all_data_RWL_4.to_csv(f'data/data_duplicates/{filename_RWL_4}', index=False)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Variable is '{variable}', must be in ['all_data_AT', 'all_data_P', 'all_data_VWC_1', 'all_data_EC_1', 'all_data_VWC_2', 'all_data_EC_2', 'all_data_GWL', 'all_data_WT', 'all_data_LT', 'all_data_GWL_csv', 'all_data_WT_csv', 'all_data_LT_csv', 'river water level 1', 'river water level 2', 'river water level 4']\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ab14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the different stations for precipitation and air temperature\n",
    "FILENAMES = ['Butschenberg.csv', 'Grundigklinik.csv', 'Hundseck.csv', 'Schafhof.csv', 'Schönbrunn.csv', 'Sportplatz.csv', \n",
    "             'Sternenberg-Schlammfang.csv', 'Schwabenquelle.csv', 'Winterberg.csv']\n",
    "\n",
    "# lists of all the different stations for soil moisture \n",
    "FILENAMES_DAT_1 = ['Schafhof1_Table1.dat', 'Schafhof5_Table1.dat']\n",
    "FILENAMES_DAT_2 = ['Schafhof1_Table2.dat', 'Schafhof5_Table2.dat']\n",
    "\n",
    "# list of all the different stations for ground water level as a xlsx file\n",
    "FILENAMES_GWL = ['Schafhof_Tensiometer.xlsx', 'Sprengquellen_Tensiometer_unten_nord.xlsx', 'Sprengquellen_Tensiometer_unten_sued.xlsx', \n",
    "                 'Sprengquellen_Tensiometer_oben_nord.xlsx', 'Sprengquellen_Tensiometer_oben_sued.xlsx']\n",
    "\n",
    "# list of all the different stations for ground water level as a csv file\n",
    "FILENAMES_GWL_csv = ['Schafhof_Tensiometer_alt.csv', 'Sprengquellen_Tensiometer_unten_nord_alt.csv', \n",
    "                     'Sprengquellen_Tensiometer_unten_sued_alt.csv', 'Sprengquellen_Tensiometer_oben_nord_alt.csv', \n",
    "                     'Sprengquellen_Tensiometer_oben_sued_alt.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_1 = ['Pegel1_Bühlot.csv', 'Pegel1_Schwabenbrünnele.csv', 'Pegel1_Büchelbach.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_2 = ['Pegel2_Bühlot.csv', 'Pegel2_Schwabenbrünnele.csv', 'Pegel2_Büchelbach.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_4 = ['Pegel4_Bühlot.csv', 'Pegel4_Schwabenbrünnele.csv', 'Pegel4_Büchelbach.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296903b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:19<00:00,  3.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:17<00:00,  3.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [00:10<00:00,  3.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:18<00:00,  3.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 49/49 [00:12<00:00,  3.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 73/73 [00:20<00:00,  3.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [00:12<00:00,  3.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [00:19<00:00,  3.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 68/68 [00:20<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing air temperature and precipitation\n",
    "for filename in FILENAMES:\n",
    "    all_data_AT = []\n",
    "    all_data_P = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for rainfall and once for the air temperature\n",
    "        df_AT = preprocessing(datafile, 'air temperature')\n",
    "        df_P = preprocessing(datafile, 'precipitation')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_AT.append(df_AT)\n",
    "        all_data_P.append(df_P)\n",
    "\n",
    "    merge('all_data_AT')\n",
    "    merge('all_data_P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0daf45d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [00:07<00:00,  6.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:09<00:00,  7.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [00:07<00:00,  5.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [00:10<00:00,  7.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing volumetric water content and electrical conductivity\n",
    "for filename in FILENAMES_DAT_1:\n",
    "    all_data_VWC_1 = []\n",
    "    all_data_EC_1 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for the volumetric water content and once for the electrical conductivity\n",
    "        df_VWC_1 = preprocessing(datafile, 'Table1_VWC')\n",
    "        df_EC_1 = preprocessing(datafile, 'Table1_EC')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_VWC_1.append(df_VWC_1)\n",
    "        all_data_EC_1.append(df_EC_1)\n",
    "\n",
    "    merge('all_data_VWC_1')\n",
    "    merge('all_data_EC_1')\n",
    "    \n",
    "for filename in FILENAMES_DAT_2:\n",
    "    all_data_VWC_2 = []\n",
    "    all_data_EC_2 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for the volumetric water content and once for the electrical conductivity\n",
    "        df_VWC_2 = preprocessing(datafile, 'Table2_VWC')\n",
    "        df_EC_2 = preprocessing(datafile, 'Table2_EC')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_VWC_2.append(df_VWC_2)\n",
    "        all_data_EC_2.append(df_EC_2)\n",
    "\n",
    "    merge('all_data_VWC_2')\n",
    "    merge('all_data_EC_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4539b3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [04:31<00:00,  6.97s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [02:09<00:00,  6.46s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:41<00:00,  4.10s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [03:18<00:00,  6.62s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [03:49<00:00,  6.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing ground water level, water temperature and logger temperature\n",
    "for filename in FILENAMES_GWL:\n",
    "    all_data_GWL = []\n",
    "    all_data_WT = []\n",
    "    all_data_LT = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_GWL = preprocessing(datafile, 'ground water level')\n",
    "        df_WT = preprocessing(datafile, 'water temperature')\n",
    "        df_LT = preprocessing(datafile, 'logger temperature')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_GWL.append(df_GWL)\n",
    "        all_data_WT.append(df_WT)\n",
    "        all_data_LT.append(df_LT)\n",
    "\n",
    "    merge('all_data_GWL')\n",
    "    merge('all_data_WT')\n",
    "    merge('all_data_LT')\n",
    "    \n",
    "# preprocessing ground water level, water temperature and logger temperature\n",
    "for filename in FILENAMES_GWL_csv:\n",
    "    all_data_GWL_csv = []\n",
    "    all_data_WT_csv = []\n",
    "    all_data_LT_csv = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_GWL_csv = preprocessing(datafile, 'ground water level csv')\n",
    "        df_WT_csv = preprocessing(datafile, 'water temperature csv')\n",
    "        df_LT_csv = preprocessing(datafile, 'logger temperature csv')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_GWL_csv.append(df_GWL_csv)\n",
    "        all_data_WT_csv.append(df_WT_csv)\n",
    "        all_data_LT_csv.append(df_LT_csv)\n",
    "\n",
    "    merge('all_data_GWL_csv')\n",
    "    merge('all_data_WT_csv')\n",
    "    merge('all_data_LT_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9eceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:09<00:00,  2.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:09<00:00,  3.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  7.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  6.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00,  6.01it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:05<00:00,  5.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00,  8.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing river water level as a csv file\n",
    "for filename in FILENAMES_RWL_1:\n",
    "    all_data_RWL_1 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_1 = preprocessing(datafile, 'river water level 1')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_1.append(df_RWL_1)\n",
    "\n",
    "    merge('all_data_RWL_1')\n",
    "\n",
    "for filename in FILENAMES_RWL_2:\n",
    "    all_data_RWL_2 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_2 = preprocessing(datafile, 'river water level 2')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_2.append(df_RWL_2)\n",
    "\n",
    "    merge('all_data_RWL_2')\n",
    "    \n",
    "for filename in FILENAMES_RWL_4:\n",
    "    all_data_RWL_4 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_4 = preprocessing(datafile, 'river water level 4')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_4.append(df_RWL_4)\n",
    "\n",
    "    merge('all_data_RWL_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94d9eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem bei diesen dateien!\n",
    "# Problem: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "# Sprenquellen_oben_nord_ground_water_level\n",
    "# Sprenquellen_oben_nord_logger_temperature\n",
    "# Sprenquellen_oben_nord_water_temperature\n",
    "# Schafhof1_volumetric_water_content_20cm\n",
    "# Schafhof1_volumetric_water_content_50cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d3dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
