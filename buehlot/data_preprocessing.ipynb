{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6600a093",
   "metadata": {},
   "source": [
    "# Bühlot data preprocessing\n",
    "\n",
    "The purpose of this code is to read in all the collected data, sort it by their different variables and then safe it in the correct folder.\n",
    "By running this code ALL the collected data will be processed, not just the new data. Therefore all the previous sorted data will be overwritten. The sorted data will be safed in a folder named \"data_export\".\n",
    "\n",
    "This is a list of all the variables:\n",
    "- air temperature [°C]\n",
    "- bulk electrical conductivity [dS/m]\n",
    "- ground water level [mm]\n",
    "- logger temperature [°C]\n",
    "- precipitation [mm]\n",
    "- river water level 1 []\n",
    "- river water level 2 []\n",
    "- river water level 4 []\n",
    "- volumetric water content [m^3/m^3]\n",
    "- water temperature [°C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "808b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f73310cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if it does not exist\n",
    "os.makedirs(\"data/data_export/air_temperature\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/bulk_electrical_conductivity\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/ground_water_level\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/logger_temperature\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/precipitation\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/river_water_level_1\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/river_water_level_2\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/river_water_level_4\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/volumetric_water_content\", exist_ok=True)\n",
    "os.makedirs(\"data/data_export/water_temperature\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "758b3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filename, variable):\n",
    "    \"\"\"\n",
    "    This function preprocesses the raw data files for the needed variable.\n",
    "    It will seperate a data file into the different variables.\n",
    "    It reads in the raw data to then create a tabel with the columns that are needed. \n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    if variable == 'precipitation':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, skiprows=1, na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'precipitation']\n",
    "        \n",
    "        # convert to datetime\n",
    "        try: \n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            \n",
    "        except ValueError:\n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%y %H:%M:%S')\n",
    "                   \n",
    "        # drop from df where precipitation is NaN\n",
    "        df.dropna(subset=[\"precipitation\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'air temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, skiprows=1, na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "        \n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'air_temperature']\n",
    "        \n",
    "        # convert to datetime      \n",
    "        try: \n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            \n",
    "        except ValueError:\n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%y %H:%M:%S')\n",
    "            \n",
    "        # drop from df where ait temperature is NaN\n",
    "        df.dropna(subset=[\"air_temperature\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'Table1_VWC':\n",
    "        \n",
    "        # read in raw data from table 1\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'volumetric_water_content']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'Table1_EC':\n",
    "        \n",
    "        # read in raw data from table 1\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'bulk_electrical_conductivity']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'Table2_VWC':\n",
    "        \n",
    "        # read in raw data from table 2\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'volumetric_water_content']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'Table2_EC':\n",
    "        \n",
    "        # read in raw data from table 2\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'bulk_electrical_conductivity']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'ground water level':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_height']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'water temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'logger temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'logger_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'ground water level csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_height']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'water temperature csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'logger temperature csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'logger_temperature']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'river water level 1':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river_water_level_1', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river_water_level_1']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "    elif variable == 'river water level 2':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river_water_level_2', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river_water_level_2']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "    elif variable == 'river water level 4':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river_water_level_4', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river_water_level_4']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Variable is '{variable}', must be in ['precipitation', 'air temperature', 'Table1_VWC', 'Table1_EC', 'Table2_VWC', 'Table2_EC', 'ground water level', 'water temperature', 'logger temperature', 'ground water level csv', 'water temperature csv', 'logger temperature csv', 'river water level 1', 'river water level 2', 'river water level 4']\")\n",
    "    \n",
    "    # return preprocessed dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3751058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(variable):\n",
    "    \"\"\"\n",
    "    This function merges all the data for the assigned list. \n",
    "    Here it is one list for the variable \"air temperature\" and one for the variable \"precipitation\". \n",
    "    It also will create a list for the sensor \"Table1\" and one for the sensor \"Table2\".\n",
    "    It will sort the lists by datetime and then safe the files in the right folder.\n",
    "    \n",
    "    \"Table1\" and \"Table2\" are names from the data file volumetric water content. Each station has two sensors (\"Table1\" and \"Table2\"). \n",
    "    While the sensor from \"Table1\" is placed in a depth of 20 cm below the top edge of the ground, the other sensor \"Table2\" is placed in a \n",
    "    depth of 50 cm below the top edge of the ground.\n",
    "    \n",
    "    The abbreviations are:\n",
    "    AT = air temperature\n",
    "    P = precipitation\n",
    "    VWC_1 = volumetric water content of \"Table1\"\n",
    "    EC_1 = bulk electrical conductivity of \"Table1\"\n",
    "    VWC_2 = volumetric water content of \"Table2\"\n",
    "    EC_2 = bulk electrical conductivity of \"Table2\"\n",
    "    GWL = ground water level\n",
    "    WT = water temperature\n",
    "    LT = logger temperature\n",
    "    GWL_csv = ground water level\n",
    "    WT_csv = water temperature\n",
    "    LG_csv = logger temperature\n",
    "    RWL_1 = river water level from the first sensor\n",
    "    RWL_2 = river water level from the second sensor\n",
    "    RWL_4 = river water level from the third sensor - sensor is named with number 4 \n",
    "    \n",
    "    \"\"\"\n",
    "       \n",
    "    if variable == 'all_data_AT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_AT = pd.concat(all_data_AT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_AT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename \n",
    "        filename_AT = filename.replace(\".csv\", \"_air_temperature.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_AT['tstamp'])==len(set(df_all_data_AT['tstamp']))\n",
    "            df_all_data_AT.to_csv(f'data/data_export/air_temperature/{filename_AT}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_AT} sind duplikate vorhanden.')\n",
    "            df_all_data_AT.to_csv(f'data/data_duplicates/{filename_AT}', index=False)\n",
    "                   \n",
    "    elif variable == 'all_data_P':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_P = pd.concat(all_data_P, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_P.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_P = filename.replace(\".csv\", \"_precipitation.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_P['tstamp'])==len(set(df_all_data_P['tstamp']))\n",
    "            df_all_data_P.to_csv(f'data/data_export/precipitation/{filename_P}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_P} sind duplikate vorhanden.')\n",
    "            df_all_data_P.to_csv(f'data/data_duplicates/{filename_P}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_VWC_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_VWC_1 = pd.concat(all_data_VWC_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_VWC_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_VWC_1 = filename.replace(\"_Table1.dat\", \"_volumetric_water_content_20cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_VWC_1['tstamp'])==len(set(df_all_data_VWC_1['tstamp']))\n",
    "            df_all_data_VWC_1.to_csv(f'data/data_export/volumetric_water_content/{filename_VWC_1}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_VWC_1} sind duplikate vorhanden.')\n",
    "            df_all_data_VWC_1.to_csv(f'data/data_duplicates/{filename_VWC_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_EC_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_EC_1 = pd.concat(all_data_EC_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_EC_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_EC_1 = filename.replace(\"_Table1.dat\", \"_bulk_electrical_conductivity_20cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_EC_1['tstamp'])==len(set(df_all_data_EC_1['tstamp']))\n",
    "            df_all_data_EC_1.to_csv(f'data/data_export/bulk_electrical_conductivity/{filename_EC_1}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_EC_1} sind duplikate vorhanden.')\n",
    "            df_all_data_EC_1.to_csv(f'data/data_duplicates/{filename_EC_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_VWC_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_VWC_2 = pd.concat(all_data_VWC_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_VWC_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_VWC_2 = filename.replace(\"_Table2.dat\", \"_volumetric_water_content_50cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_VWC_2['tstamp'])==len(set(df_all_data_VWC_2['tstamp']))\n",
    "            df_all_data_VWC_2.to_csv(f'data/data_export/volumetric_water_content/{filename_VWC_2}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_VWC_2} sind duplikate vorhanden.')\n",
    "            df_all_data_VWC_2.to_csv(f'data/data_duplicates/{filename_VWC_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_EC_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_EC_2 = pd.concat(all_data_EC_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_EC_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_EC_2 = filename.replace(\"_Table2.dat\", \"_bulk_electrical_conductivity_50cm.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_EC_2['tstamp'])==len(set(df_all_data_EC_2['tstamp']))\n",
    "            df_all_data_EC_2.to_csv(f'data/data_export/bulk_electrical_conductivity/{filename_EC_2}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_EC_2} sind duplikate vorhanden.')\n",
    "            df_all_data_EC_2.to_csv(f'data/data_duplicates/{filename_EC_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_GWL':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_GWL = pd.concat(all_data_GWL, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_GWL.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_GWL = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_ground_water_level.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_GWL['tstamp'])==len(set(df_all_data_GWL['tstamp']))\n",
    "            df_all_data_GWL.to_csv(f'data/data_export/ground_water_level/{filename_GWL}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_GWL} sind duplikate vorhanden.')\n",
    "            df_all_data_GWL.to_csv(f'data/data_duplicates/{filename_GWL}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_WT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_WT = pd.concat(all_data_WT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_WT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_WT = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_water_temperature.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_WT['tstamp'])==len(set(df_all_data_WT['tstamp']))\n",
    "            df_all_data_WT.to_csv(f'data/data_export/water_temperature/{filename_WT}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_WT} sind duplikate vorhanden.')\n",
    "            df_all_data_WT.to_csv(f'data/data_duplicates/{filename_WT}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_LT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_LT = pd.concat(all_data_LT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_LT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_LT = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_logger_temperature.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_LT['tstamp'])==len(set(df_all_data_LT['tstamp']))\n",
    "            df_all_data_LT.to_csv(f'data/data_export/logger_temperature/{filename_LT}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_LT} sind duplikate vorhanden.')\n",
    "            df_all_data_LT.to_csv(f'data/data_duplicates/{filename_LT}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_GWL_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_GWL_csv = pd.concat(all_data_GWL_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_GWL_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_GWL_csv = filename.replace(\".csv\", \"_GWL.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_GWL_csv['tstamp'])==len(set(df_all_data_GWL_csv['tstamp']))\n",
    "            df_all_data_GWL_csv.to_csv(f'data/data_export/ground_water_level/{filename_GWL_csv}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_GWL_csv} sind duplikate vorhanden.')\n",
    "            df_all_data_GWL_csv.to_csv(f'data/data_duplicates/{filename_GWL_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_WT_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_WT_csv = pd.concat(all_data_WT_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_WT_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_WT_csv = filename.replace(\".csv\", \"_WT.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_WT_csv['tstamp'])==len(set(df_all_data_WT_csv['tstamp']))\n",
    "            df_all_data_WT_csv.to_csv(f'data/data_export/water_temperature/{filename_WT_csv}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_WT_csv} sind duplikate vorhanden.')\n",
    "            df_all_data_WT_csv.to_csv(f'data/data_duplicates/{filename_WT_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_LT_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_LT_csv = pd.concat(all_data_LT_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_LT_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_LT_csv = filename.replace(\".csv\", \"_LT.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_LT_csv['tstamp'])==len(set(df_all_data_LT_csv['tstamp']))\n",
    "            df_all_data_LT_csv.to_csv(f'data/data_export/logger_temperature/{filename_LT_csv}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_LT_csv} sind duplikate vorhanden.')\n",
    "            df_all_data_LT_csv.to_csv(f'data/data_duplicates/{filename_LT_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_1 = pd.concat(all_data_RWL_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_1 = filename.replace(\"Pegel1_\", \"\").replace(\".csv\", \"_river_water_level_1.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_RWL_1['tstamp'])==len(set(df_all_data_RWL_1['tstamp']))\n",
    "            df_all_data_RWL_1.to_csv(f'data/data_export/river_water_level_1/{filename_RWL_1}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_RWL_1} sind duplikate vorhanden.')\n",
    "            df_all_data_RWL_1.to_csv(f'data/data_duplicates/{filename_RWL_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_2 = pd.concat(all_data_RWL_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_2 = filename.replace(\"Pegel2_\", \"\").replace(\".csv\", \"_river_water_level_2.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_RWL_2['tstamp'])==len(set(df_all_data_RWL_2['tstamp']))\n",
    "            df_all_data_RWL_2.to_csv(f'data/data_export/river_water_level_2/{filename_RWL_2}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_RWL_2} sind duplikate vorhanden.')\n",
    "            df_all_data_RWL_2.to_csv(f'data/data_duplicates/{filename_RWL_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_4':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_4 = pd.concat(all_data_RWL_4, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_4.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_4 = filename.replace(\"Pegel4_\", \"\").replace(\".csv\", \"_river_water_level_4.csv\")\n",
    "        \n",
    "        # check if there are any duplicates - if yes safe that file in folder 'data_duplicates' \n",
    "        try:\n",
    "            assert len(df_all_data_RWL_4['tstamp'])==len(set(df_all_data_RWL_4['tstamp']))\n",
    "            df_all_data_RWL_4.to_csv(f'data/data_export/river_water_level_4/{filename_RWL_4}', index=False)\n",
    "        except AssertionError:\n",
    "            print(f'Bei datei {filename_RWL_4} sind duplikate vorhanden.')\n",
    "            df_all_data_RWL_4.to_csv(f'data/data_duplicates/{filename_RWL_4}', index=False)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Variable is '{variable}', must be in ['all_data_AT', 'all_data_P', 'all_data_VWC_1', 'all_data_EC_1', 'all_data_VWC_2', 'all_data_EC_2', 'all_data_GWL', 'all_data_WT', 'all_data_LT', 'all_data_GWL_csv', 'all_data_WT_csv', 'all_data_LT_csv', 'river water level 1', 'river water level 2', 'river water level 4']\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05ab14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the different stations for precipitation and air temperature\n",
    "FILENAMES = ['Butschenberg.csv', 'Grundigklinik.csv', 'Hundseck.csv', 'Schafhof.csv', 'Schönbrunn.csv', 'Sportplatz.csv', \n",
    "             'Sternenberg-Schlammfang.csv', 'Schwabenquelle.csv', 'Winterberg.csv']\n",
    "\n",
    "# lists of all the different stations for soil moisture \n",
    "FILENAMES_DAT_1 = ['Schafhof1_Table1.dat', 'Schafhof5_Table1.dat']\n",
    "FILENAMES_DAT_2 = ['Schafhof1_Table2.dat', 'Schafhof5_Table2.dat']\n",
    "\n",
    "# list of all the different stations for ground water level as a xlsx file\n",
    "FILENAMES_GWL = ['Schafhof_Tensiometer.xlsx', 'Sprengquellen_Tensiometer_unten_nord.xlsx', 'Sprengquellen_Tensiometer_unten_sued.xlsx', \n",
    "                 'Sprengquellen_Tensiometer_oben_nord.xlsx', 'Sprengquellen_Tensiometer_oben_sued.xlsx']\n",
    "\n",
    "# list of all the different stations for ground water level as a csv file\n",
    "FILENAMES_GWL_csv = ['Schafhof_Tensiometer_alt.csv', 'Sprengquellen_Tensiometer_unten_nord_alt.csv', \n",
    "                     'Sprengquellen_Tensiometer_unten_sued_alt.csv', 'Sprengquellen_Tensiometer_oben_nord_alt.csv', \n",
    "                     'Sprengquellen_Tensiometer_oben_sued_alt.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_1 = ['Pegel1_Bühlot.csv', 'Pegel1_Schwabenbrünnele.csv', 'Pegel1_Büchelbach.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_2 = ['Pegel2_Bühlot.csv', 'Pegel2_Schwabenbrünnele.csv', 'Pegel2_Büchelbach.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_4 = ['Pegel4_Bühlot.csv', 'Pegel4_Schwabenbrünnele.csv', 'Pegel4_Büchelbach.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "296903b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [00:06<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Butschenberg_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:06<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Grundigklinik_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Grundigklinik_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [00:03<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Hundseck_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Hundseck_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:06<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schafhof_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Schafhof_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schönbrunn_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Schönbrunn_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:06<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Sportplatz_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Sportplatz_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [00:04<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Sternenberg-Schlammfang_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Sternenberg-Schlammfang_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:06<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schwabenquelle_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Schwabenquelle_precipitation.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:06<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Winterberg_air_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Winterberg_precipitation.csv sind duplikate vorhanden.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing air temperature and precipitation\n",
    "for filename in FILENAMES:\n",
    "    all_data_AT = []\n",
    "    all_data_P = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for rainfall and once for the air temperature\n",
    "        df_AT = preprocessing(datafile, 'air temperature')\n",
    "        df_P = preprocessing(datafile, 'precipitation')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_AT.append(df_AT)\n",
    "        all_data_P.append(df_P)\n",
    "\n",
    "    merge('all_data_AT')\n",
    "    merge('all_data_P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0daf45d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [00:02<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schafhof1_volumetric_water_content_20cm.csv sind duplikate vorhanden.\n",
      "Bei datei Schafhof1_bulk_electrical_conductivity_20cm.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 72/72 [00:03<00:00, 21.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schafhof5_volumetric_water_content_20cm.csv sind duplikate vorhanden.\n",
      "Bei datei Schafhof5_bulk_electrical_conductivity_20cm.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schafhof1_volumetric_water_content_50cm.csv sind duplikate vorhanden.\n",
      "Bei datei Schafhof1_bulk_electrical_conductivity_50cm.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 71/71 [00:03<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schafhof5_volumetric_water_content_50cm.csv sind duplikate vorhanden.\n",
      "Bei datei Schafhof5_bulk_electrical_conductivity_50cm.csv sind duplikate vorhanden.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing volumetric water content and electrical conductivity\n",
    "for filename in FILENAMES_DAT_1:\n",
    "    all_data_VWC_1 = []\n",
    "    all_data_EC_1 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for the volumetric water content and once for the electrical conductivity\n",
    "        df_VWC_1 = preprocessing(datafile, 'Table1_VWC')\n",
    "        df_EC_1 = preprocessing(datafile, 'Table1_EC')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_VWC_1.append(df_VWC_1)\n",
    "        all_data_EC_1.append(df_EC_1)\n",
    "\n",
    "    merge('all_data_VWC_1')\n",
    "    merge('all_data_EC_1')\n",
    "    \n",
    "for filename in FILENAMES_DAT_2:\n",
    "    all_data_VWC_2 = []\n",
    "    all_data_EC_2 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for the volumetric water content and once for the electrical conductivity\n",
    "        df_VWC_2 = preprocessing(datafile, 'Table2_VWC')\n",
    "        df_EC_2 = preprocessing(datafile, 'Table2_EC')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_VWC_2.append(df_VWC_2)\n",
    "        all_data_EC_2.append(df_EC_2)\n",
    "\n",
    "    merge('all_data_VWC_2')\n",
    "    merge('all_data_EC_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4539b3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [01:43<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schafhof_ground_water_level.csv sind duplikate vorhanden.\n",
      "Bei datei Schafhof_water_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Schafhof_logger_temperature.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:47<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Sprengquellen_unten_nord_ground_water_level.csv sind duplikate vorhanden.\n",
      "Bei datei Sprengquellen_unten_nord_water_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Sprengquellen_unten_nord_logger_temperature.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:15<00:00,  1.56s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:15<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Sprengquellen_oben_nord_ground_water_level.csv sind duplikate vorhanden.\n",
      "Bei datei Sprengquellen_oben_nord_water_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Sprengquellen_oben_nord_logger_temperature.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [01:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Sprengquellen_oben_süd_ground_water_level.csv sind duplikate vorhanden.\n",
      "Bei datei Sprengquellen_oben_süd_water_temperature.csv sind duplikate vorhanden.\n",
      "Bei datei Sprengquellen_oben_süd_logger_temperature.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 10.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing ground water level, water temperature and logger temperature as a xlsx file\n",
    "for filename in FILENAMES_GWL:\n",
    "    all_data_GWL = []\n",
    "    all_data_WT = []\n",
    "    all_data_LT = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_GWL = preprocessing(datafile, 'ground water level')\n",
    "        df_WT = preprocessing(datafile, 'water temperature')\n",
    "        df_LT = preprocessing(datafile, 'logger temperature')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_GWL.append(df_GWL)\n",
    "        all_data_WT.append(df_WT)\n",
    "        all_data_LT.append(df_LT)\n",
    "\n",
    "    merge('all_data_GWL')\n",
    "    merge('all_data_WT')\n",
    "    merge('all_data_LT')\n",
    "    \n",
    "# preprocessing ground water level, water temperature and logger temperature as a csv file\n",
    "for filename in FILENAMES_GWL_csv:\n",
    "    all_data_GWL_csv = []\n",
    "    all_data_WT_csv = []\n",
    "    all_data_LT_csv = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_GWL_csv = preprocessing(datafile, 'ground water level csv')\n",
    "        df_WT_csv = preprocessing(datafile, 'water temperature csv')\n",
    "        df_LT_csv = preprocessing(datafile, 'logger temperature csv')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_GWL_csv.append(df_GWL_csv)\n",
    "        all_data_WT_csv.append(df_WT_csv)\n",
    "        all_data_LT_csv.append(df_LT_csv)\n",
    "\n",
    "    merge('all_data_GWL_csv')\n",
    "    merge('all_data_WT_csv')\n",
    "    merge('all_data_LT_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b9eceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Bühlot_river_water_level_1.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schwabenbrünnele_river_water_level_1.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Büchelbach_river_water_level_1.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Bühlot_river_water_level_2.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:01<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schwabenbrünnele_river_water_level_2.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 22.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Büchelbach_river_water_level_2.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Bühlot_river_water_level_4.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:01<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Schwabenbrünnele_river_water_level_4.csv sind duplikate vorhanden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 27.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei datei Büchelbach_river_water_level_4.csv sind duplikate vorhanden.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing river water level as a csv file\n",
    "for filename in FILENAMES_RWL_1:\n",
    "    all_data_RWL_1 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_1 = preprocessing(datafile, 'river water level 1')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_1.append(df_RWL_1)\n",
    "\n",
    "    merge('all_data_RWL_1')\n",
    "\n",
    "for filename in FILENAMES_RWL_2:\n",
    "    all_data_RWL_2 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_2 = preprocessing(datafile, 'river water level 2')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_2.append(df_RWL_2)\n",
    "\n",
    "    merge('all_data_RWL_2')\n",
    "    \n",
    "for filename in FILENAMES_RWL_4:\n",
    "    all_data_RWL_4 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_4 = preprocessing(datafile, 'river water level 4')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_4.append(df_RWL_4)\n",
    "\n",
    "    merge('all_data_RWL_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f38f7aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713109\n",
      "702744\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/data_duplicates/Schafhof_precipitation.csv')\n",
    "print(len(df['tstamp']))\n",
    "print(len(set(df['tstamp'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e75634fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tstamp</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-13 09:05:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-13 09:10:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-13 09:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-03-13 09:20:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-13 09:25:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713104</th>\n",
       "      <td>2023-05-23 07:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713105</th>\n",
       "      <td>2023-05-23 07:05:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713106</th>\n",
       "      <td>2023-05-23 07:10:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713107</th>\n",
       "      <td>2023-05-23 07:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713108</th>\n",
       "      <td>2023-05-23 07:20:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>713109 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tstamp  precipitation\n",
       "0       2015-03-13 09:05:00            0.0\n",
       "1       2015-03-13 09:10:00            0.0\n",
       "2       2015-03-13 09:15:00            0.0\n",
       "3       2015-03-13 09:20:00            0.0\n",
       "4       2015-03-13 09:25:00            0.0\n",
       "...                     ...            ...\n",
       "713104  2023-05-23 07:00:00            0.0\n",
       "713105  2023-05-23 07:05:00            0.0\n",
       "713106  2023-05-23 07:10:00            0.0\n",
       "713107  2023-05-23 07:15:00            0.0\n",
       "713108  2023-05-23 07:20:00            0.0\n",
       "\n",
       "[713109 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/data_duplicates/Schafhof_precipitation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9eb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
