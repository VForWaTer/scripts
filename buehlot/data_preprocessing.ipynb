{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6600a093",
   "metadata": {},
   "source": [
    "# Bühlot data preprocessing\n",
    "\n",
    "The purpose of this code is to read in all the collected data, sort it by their different variables and then safe it in the correct folder.\n",
    "By running this code ALL the collected data will be processed, not just the new data. Therefore all the previous sorted data will be overwritten. The sorted data will be safed in a folder named \"data_export\".\n",
    "\n",
    "This is a list of all the variables:\n",
    "- air temperature [°C]\n",
    "- bulk electrical conductivity [dS/m]\n",
    "- ground water level [mm]\n",
    "- logger temperature [°C]\n",
    "- precipitation [mm]\n",
    "- river water level 1 []\n",
    "- river water level 2 []\n",
    "- river water level 4 []\n",
    "- volumetric water content [m^3/m^3; %]\n",
    "- water temperature [°C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "758b3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filename, variable):\n",
    "    \"\"\"\n",
    "    This function preprocesses the raw data files for the needed variable.\n",
    "    It will seperate a data file into the different variables.\n",
    "    It reads in the raw data to then create a tabel with the columns that are needed. \n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    if variable == 'precipitation':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, skiprows=1, na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'precipitation [mm]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        try: \n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            \n",
    "        except ValueError:\n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%y %H:%M:%S')\n",
    "            \n",
    "        #finally:\n",
    "            #df['tstamp'] = pd.to_datetime(df['tstamp'])\n",
    "        \n",
    "        # drop from df where precipitation is NaN\n",
    "        df.dropna(subset=[\"precipitation [mm]\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'air temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, skiprows=1, na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "        \n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'air temperature [°C]']\n",
    "        \n",
    "        # convert to datetime      \n",
    "        try: \n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%m/%d/%y %I:%M:%S %p')\n",
    "            \n",
    "        except ValueError:\n",
    "            df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%y %H:%M:%S')\n",
    "            \n",
    "        #finally:\n",
    "            #df['tstamp'] = pd.to_datetime(df['tstamp'])\n",
    "            \n",
    "        # drop from df where ait temperature is NaN\n",
    "        df.dropna(subset=[\"air temperature [°C]\"], inplace=True)\n",
    "        \n",
    "    elif variable == 'Table1_VWC':\n",
    "        \n",
    "        # read in raw data from table 1\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "        \n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'volumetric water content [m^3/m^3; %]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'Table1_EC':\n",
    "        \n",
    "        # read in raw data from table 1\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'bulk electrical conductivity [dS/m]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'Table2_VWC':\n",
    "        \n",
    "        # read in raw data from table 2\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'volumetric water content [m^3/m^3; %]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'Table2_EC':\n",
    "        \n",
    "        # read in raw data from table 2\n",
    "        df = pd.read_csv(filename, skiprows=[1,2,3,4], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [0,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'bulk electrical conductivity [dS/m]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'ground water level':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water height [mm]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'water temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water temperature [°C]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'logger temperature':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_excel(filename, skiprows=[1,2,3,4,5,6,7,8,9,10,11], na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'logger temperature [°C]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'ground water level csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,4]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water height [mm]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'water temperature csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,2]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'water temperature [°C]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'logger temperature csv':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged')\n",
    "\n",
    "        # slice down to relevant columns\n",
    "        df = df.iloc[:, [1,3]].copy()\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['tstamp', 'logger temperature [°C]']\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "    elif variable == 'river water level 1':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river water level 1 []', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river water level 1 []']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "    elif variable == 'river water level 2':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river water level 2 []', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river water level 2 []']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "    elif variable == 'river water level 4':\n",
    "        \n",
    "        # read in raw data\n",
    "        df = pd.read_csv(filename, na_values='Logged', sep=';', header=None)\n",
    "\n",
    "        # merge date with time\n",
    "        df['tstamp'] = df.iloc[:,0] + ' ' + df.iloc[:,1]\n",
    "\n",
    "        # rename columns\n",
    "        df.columns = ['date_str', 'time', 'river water level 4 []', 'tstamp']\n",
    "\n",
    "        # change the order of the columns\n",
    "        df = df[['tstamp', 'river water level 4 []']]\n",
    "        \n",
    "        # convert to datetime\n",
    "        df['tstamp'] = pd.to_datetime(df['tstamp'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Variable is '{variable}', must be in ['precipitation', 'air temperature', 'Table1_VWC', 'Table1_EC', 'Table2_VWC', 'Table2_EC', 'ground water level', 'water temperature', 'logger temperature', 'ground water level csv', 'water temperature csv', 'logger temperature csv', 'river water level 1', 'river water level 2', 'river water level 4']\")\n",
    "    \n",
    "    # return preprocessed dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3751058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(variable):\n",
    "    \"\"\"\n",
    "    This function merges all the data for the assigned list. \n",
    "    Here it is one list for the variable \"air temperature\" and one for the variable \"precipitation\". \n",
    "    It also will create a list for the sensor \"Table1\" and one for the sensor \"Table2\".\n",
    "    It will sort the lists by datetime and then safe the files in the right folder.\n",
    "    \n",
    "    \"Table1\" and \"Table2\" are names from the data file volumetric water content. Each station has two sensors (\"Table1\" and \"Table2\"). \n",
    "    While the sensor from \"Table1\" is placed in a depth of 20 cm below the top edge of the ground, the other sensor \"Table2\" is placed in a \n",
    "    depth of 50 cm below the top edge of the ground.\n",
    "    \n",
    "    The abbreviations are:\n",
    "    AT = air temperature\n",
    "    P = precipitation\n",
    "    VWC_1 = volumetric water content of \"Table1\"\n",
    "    EC_1 = bulk electrical conductivity of \"Table1\"\n",
    "    VWC_2 = volumetric water content of \"Table2\"\n",
    "    EC_2 = bulk electrical conductivity of \"Table2\"\n",
    "    GWL = ground water level as a .xlsx file\n",
    "    WT = water temperature as a .xlsx file\n",
    "    LT = logger temperature as a .xlsx file\n",
    "    GWL_csv = ground water level as a .csv file\n",
    "    WT_csv = water temperature as a .csv file\n",
    "    LG_csv = logger temperature as a .csv file \n",
    "    RWL_1 = river water level from the first sensor\n",
    "    RWL_2 = river water level from the second sensor\n",
    "    RWL_4 = river water level from the third sensor - sensor is named with number 4 \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if variable == 'all_data_AT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_AT = pd.concat(all_data_AT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_AT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename \n",
    "        filename_AT = filename.replace(\".csv\", \"_air_temperature.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_AT.to_csv(f'data/data_export/air_temperature/{filename_AT}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_P':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_P = pd.concat(all_data_P, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_P.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_P = filename.replace(\".csv\", \"_precipitation.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_P.to_csv(f'data/data_export/precipitation/{filename_P}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_VWC_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_VWC_1 = pd.concat(all_data_VWC_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_VWC_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_VWC_1 = filename.replace(\"_Table1.dat\", \"_volumetric_water_content_20cm.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_VWC_1.to_csv(f'data/data_export/volumetric_water_content/{filename_VWC_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_EC_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_EC_1 = pd.concat(all_data_EC_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_EC_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_EC_1 = filename.replace(\"_Table1.dat\", \"_bulk_electrical_conductivity_20cm.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_EC_1.to_csv(f'data/data_export/bulk_electrical_conductivity/{filename_EC_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_VWC_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_VWC_2 = pd.concat(all_data_VWC_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_VWC_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_VWC_2 = filename.replace(\"_Table2.dat\", \"_volumetric_water_content_50cm.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_VWC_2.to_csv(f'data/data_export/volumetric_water_content/{filename_VWC_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_EC_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_EC_2 = pd.concat(all_data_EC_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_EC_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_EC_2 = filename.replace(\"_Table2.dat\", \"_bulk_electrical_conductivity_50cm.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_EC_2.to_csv(f'data/data_export/bulk_electrical_conductivity/{filename_EC_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_GWL':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_GWL = pd.concat(all_data_GWL, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_GWL.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_GWL = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_ground_water_level.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_GWL.to_csv(f'data/data_export/ground_water_level/{filename_GWL}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_WT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_WT = pd.concat(all_data_WT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_WT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_WT = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_water_temperature.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_WT.to_csv(f'data/data_export/water_temperature/{filename_WT}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_LT':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_LT = pd.concat(all_data_LT, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_LT.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_LT = filename.replace(\"_Tensiometer\", \"\").replace(\"sued\", \"süd\").replace(\".xlsx\", \"_logger_temperature.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_LT.to_csv(f'data/data_export/logger_temperature/{filename_LT}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_GWL_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_GWL_csv = pd.concat(all_data_GWL_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_GWL_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_GWL_csv = filename.replace(\".csv\", \"_GWL.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_GWL_csv.to_csv(f'data/data_export/ground_water_level/{filename_GWL_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_WT_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_WT_csv = pd.concat(all_data_WT_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_WT_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_WT_csv = filename.replace(\".csv\", \"_WT.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_WT_csv.to_csv(f'data/data_export/water_temperature/{filename_WT_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_LT_csv':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_LT_csv = pd.concat(all_data_LT_csv, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_LT_csv.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_LT_csv = filename.replace(\".csv\", \"_LT.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_LT_csv.to_csv(f'data/data_export/logger_temperature/{filename_LT_csv}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_1':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_1 = pd.concat(all_data_RWL_1, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_1.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_1 = filename.replace(\"Pegel1_\", \"\").replace(\".csv\", \"_river_water_level_1.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_RWL_1.to_csv(f'data/data_export/river_water_level_1/{filename_RWL_1}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_2':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_2 = pd.concat(all_data_RWL_2, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_2.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_2 = filename.replace(\"Pegel2_\", \"\").replace(\".csv\", \"_river_water_level_2.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_RWL_2.to_csv(f'data/data_export/river_water_level_2/{filename_RWL_2}', index=False)\n",
    "        \n",
    "    elif variable == 'all_data_RWL_4':\n",
    "        \n",
    "        # merge all_data\n",
    "        df_all_data_RWL_4 = pd.concat(all_data_RWL_4, ignore_index=True)\n",
    "        \n",
    "        # sort by datetime\n",
    "        df_all_data_RWL_4.sort_index(axis='index', inplace=False)\n",
    "        \n",
    "        # replace filename\n",
    "        filename_RWL_4 = filename.replace(\"Pegel4_\", \"\").replace(\".csv\", \"_river_water_level_4.csv\")\n",
    "        \n",
    "        # safe file to csv in the right folder\n",
    "        df_all_data_RWL_4.to_csv(f'data/data_export/river_water_level_4/{filename_RWL_4}', index=False)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Variable is '{variable}', must be in ['all_data_AT', 'all_data_P', 'all_data_VWC_1', 'all_data_EC_1', 'all_data_VWC_2', 'all_data_EC_2', 'all_data_GWL', 'all_data_WT', 'all_data_LT', 'all_data_GWL_csv', 'all_data_WT_csv', 'all_data_LT_csv', 'river water level 1', 'river water level 2', 'river water level 4']\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05ab14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the different stations for precipitation and air temperature\n",
    "FILENAMES = ['Butschenberg.csv', 'Grundigklinik.csv', 'Hundseck.csv', 'Schafhof.csv', 'Schönbrunn.csv', 'Sportplatz.csv', \n",
    "             'Sternenberg-Schlammfang.csv', 'Schwabenquelle.csv', 'Winterberg.csv']\n",
    "\n",
    "# lists of all the different stations for soil moisture \n",
    "FILENAMES_DAT_1 = ['Schafhof1_Table1.dat', 'Schafhof5_Table1.dat']\n",
    "FILENAMES_DAT_2 = ['Schafhof1_Table2.dat', 'Schafhof5_Table2.dat']\n",
    "\n",
    "# list of all the different stations for ground water level as a xlsx file\n",
    "FILENAMES_GWL = ['Schafhof_Tensiometer.xlsx', 'Sprengquellen_Tensiometer_unten_nord.xlsx', 'Sprengquellen_Tensiometer_unten_sued.xlsx', \n",
    "                 'Sprengquellen_Tensiometer_oben_nord.xlsx', 'Sprengquellen_Tensiometer_oben_sued.xlsx']\n",
    "\n",
    "# list of all the different stations for ground water level as a csv file\n",
    "FILENAMES_GWL_csv = ['Schafhof_Tensiometer_alt.csv', 'Sprengquellen_Tensiometer_unten_nord_alt.csv', \n",
    "                     'Sprengquellen_Tensiometer_unten_sued_alt.csv', 'Sprengquellen_Tensiometer_oben_nord_alt.csv', \n",
    "                     'Sprengquellen_Tensiometer_oben_sued_alt.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_1 = ['Pegel1_Bühlot.csv', 'Pegel1_Schwabenbrünnele.csv', 'Pegel1_Büchelbach.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_2 = ['Pegel2_Bühlot.csv', 'Pegel2_Schwabenbrünnele.csv', 'Pegel2_Büchelbach.csv']\n",
    "\n",
    "# list of all the different stations for river water level as a csv file\n",
    "FILENAMES_RWL_4 = ['Pegel4_Bühlot.csv', 'Pegel4_Schwabenbrünnele.csv', 'Pegel4_Büchelbach.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "296903b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 62/62 [00:06<00:00,  9.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:05<00:00, 11.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [00:03<00:00,  9.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:06<00:00, 10.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 10.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 69/69 [00:06<00:00, 10.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [00:04<00:00, 10.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 59/59 [00:06<00:00,  9.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:06<00:00,  9.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing air temperature and precipitation\n",
    "for filename in FILENAMES:\n",
    "    all_data_AT = []\n",
    "    all_data_P = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for rainfall and once for the air temperature\n",
    "        df_AT = preprocessing(datafile, 'air temperature')\n",
    "        df_P = preprocessing(datafile, 'precipitation')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_AT.append(df_AT)\n",
    "        all_data_P.append(df_P)\n",
    "\n",
    "    merge('all_data_AT')\n",
    "    merge('all_data_P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0daf45d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/47 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9980\\3966586145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# preprocess each raw data file, once for the volumetric water content and once for the electrical conductivity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdf_VWC_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Table1_VWC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mdf_EC_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Table1_EC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# append to all_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9980\\3637242671.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(filename, variable)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# slice down to relevant columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# rename columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m         \u001b[0mtup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_tuple_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;31m# check that the key does not exceed the maximum size of the index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Can only index by location with a [{self._valid_types}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# preprocessing volumetric water content and electrical conductivity\n",
    "for filename in FILENAMES_DAT_1:\n",
    "    all_data_VWC_1 = []\n",
    "    all_data_EC_1 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for the volumetric water content and once for the electrical conductivity\n",
    "        df_VWC_1 = preprocessing(datafile, 'Table1_VWC')\n",
    "        df_EC_1 = preprocessing(datafile, 'Table1_EC')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_VWC_1.append(df_VWC_1)\n",
    "        all_data_EC_1.append(df_EC_1)\n",
    "\n",
    "    merge('all_data_VWC_1')\n",
    "    merge('all_data_EC_1')\n",
    "    \n",
    "for filename in FILENAMES_DAT_2:\n",
    "    all_data_VWC_2 = []\n",
    "    all_data_EC_2 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file, once for the volumetric water content and once for the electrical conductivity\n",
    "        df_VWC_2 = preprocessing(datafile, 'Table2_VWC')\n",
    "        df_EC_2 = preprocessing(datafile, 'Table2_EC')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_VWC_2.append(df_VWC_2)\n",
    "        all_data_EC_2.append(df_EC_2)\n",
    "\n",
    "    merge('all_data_VWC_2')\n",
    "    merge('all_data_EC_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4539b3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [01:39<00:00,  2.56s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:47<00:00,  2.36s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:15<00:00,  1.53s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:13<00:00,  2.46s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [01:25<00:00,  2.60s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing ground water level, water temperature and logger temperature as a xlsx file\n",
    "for filename in FILENAMES_GWL:\n",
    "    all_data_GWL = []\n",
    "    all_data_WT = []\n",
    "    all_data_LT = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_GWL = preprocessing(datafile, 'ground water level')\n",
    "        df_WT = preprocessing(datafile, 'water temperature')\n",
    "        df_LT = preprocessing(datafile, 'logger temperature')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_GWL.append(df_GWL)\n",
    "        all_data_WT.append(df_WT)\n",
    "        all_data_LT.append(df_LT)\n",
    "\n",
    "    merge('all_data_GWL')\n",
    "    merge('all_data_WT')\n",
    "    merge('all_data_LT')\n",
    "    \n",
    "# preprocessing ground water level, water temperature and logger temperature as a csv file\n",
    "for filename in FILENAMES_GWL_csv:\n",
    "    all_data_GWL_csv = []\n",
    "    all_data_WT_csv = []\n",
    "    all_data_LT_csv = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_GWL_csv = preprocessing(datafile, 'ground water level csv')\n",
    "        df_WT_csv = preprocessing(datafile, 'water temperature csv')\n",
    "        df_LT_csv = preprocessing(datafile, 'logger temperature csv')\n",
    "        \n",
    "        # append to all_data\n",
    "        all_data_GWL_csv.append(df_GWL_csv)\n",
    "        all_data_WT_csv.append(df_WT_csv)\n",
    "        all_data_LT_csv.append(df_LT_csv)\n",
    "\n",
    "    merge('all_data_GWL_csv')\n",
    "    merge('all_data_WT_csv')\n",
    "    merge('all_data_LT_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b9eceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  5.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:03<00:00,  9.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 19.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 16.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:02<00:00, 14.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 18.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 13.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:02<00:00, 14.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 22.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing river water level as a csv file\n",
    "for filename in FILENAMES_RWL_1:\n",
    "    all_data_RWL_1 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_1 = preprocessing(datafile, 'river water level 1')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_1.append(df_RWL_1)\n",
    "\n",
    "    merge('all_data_RWL_1')\n",
    "\n",
    "for filename in FILENAMES_RWL_2:\n",
    "    all_data_RWL_2 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_2 = preprocessing(datafile, 'river water level 2')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_2.append(df_RWL_2)\n",
    "\n",
    "    merge('all_data_RWL_2')\n",
    "    \n",
    "for filename in FILENAMES_RWL_4:\n",
    "    all_data_RWL_4 = []\n",
    "    \n",
    "    filenames = glob(f\"*/*/*/{filename}\", recursive=False)\n",
    "    for datafile in tqdm(filenames):\n",
    "        \n",
    "        # preprocess each raw data file\n",
    "        df_RWL_4 = preprocessing(datafile, 'river water level 4')\n",
    "\n",
    "        # append to all_data\n",
    "        all_data_RWL_4.append(df_RWL_4)\n",
    "\n",
    "    merge('all_data_RWL_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f7aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
